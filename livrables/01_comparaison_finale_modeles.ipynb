{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparaison des Mod√®les - Analyse de Sentiments Twitter\n\nCe notebook compare les diff√©rents mod√®les d'analyse de sentiments d√©velopp√©s pour le projet Air Paradis.\n\n---\n\n## Table des mati√®res\n\n1. [Introduction et Contexte](#1-introduction)\n2. [Rappel des Crit√®res d'√âvaluation](#2-criteres)\n3. [Synth√®se des Exp√©rimentations](#3-synthese)\n4. [Comparaison des Performances](#4-comparaison)\n5. [Analyse Approfondie](#5-analyse)\n6. [S√©lection du Mod√®le Final](#6-selection)\n7. [Conclusion](#7-conclusion)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et Contexte <a id=\"1-introduction\"></a>\n",
    "\n",
    "### Contexte M√©tier\n",
    "\n",
    "**Air Paradis**, compagnie a√©rienne, souhaite anticiper les bad buzz sur les r√©seaux sociaux en d√©veloppant un produit IA capable de pr√©dire le sentiment des tweets en temps r√©el.\n",
    "\n",
    "### Objectifs du Projet\n",
    "\n",
    "1. D√©velopper **3 approches** de mod√©lisation :\n",
    "   - **Approche simple** : Mod√®le classique (R√©gression Logistique)\n",
    "   - **Approche avanc√©e** : Deep Learning avec embeddings (LSTM, CNN)\n",
    "   - **Approche BERT** : Transfer Learning avec mod√®le pr√©-entra√Æn√©\n",
    "\n",
    "2. Mettre en ≈ìuvre une **d√©marche MLOps compl√®te** :\n",
    "   - Tracking avec MLFlow\n",
    "   - Versioning avec Git/GitHub\n",
    "   - CI/CD avec tests automatis√©s\n",
    "   - Monitoring en production avec Azure Application Insights\n",
    "\n",
    "3. D√©ployer le meilleur mod√®le via une **API REST sur le Cloud**\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Source** : Sentiment140 (tweets annot√©s)\n",
    "- **Taille originale** : 1,600,000 tweets\n",
    "- **Apr√®s nettoyage** : 1,385,537 tweets\n",
    "- **Distribution** : √âquilibr√©e (50% positif, 50% n√©gatif)\n",
    "- **Splits** : Train (70%), Validation (15%), Test (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rappel des Crit√®res d'√âvaluation <a id=\"2-criteres\"></a>\n",
    "\n",
    "### Crit√®res d'√âvaluation CE1 - Embeddings\n",
    "\n",
    "‚úÖ **Requis** :\n",
    "- Tester **au moins 2 techniques de pr√©traitement** (lemmatization, stemming)\n",
    "- Tester **au moins 2 m√©thodes d'embedding** parmi Word2Vec, GloVe, FastText\n",
    "- Pr√©parer les donn√©es pour **BERT** (input_ids, attention_mask)\n",
    "- (Optionnel) Tester **USE** (Universal Sentence Encoder)\n",
    "\n",
    "### Crit√®res d'√âvaluation CE6 - Architectures\n",
    "\n",
    "‚úÖ **Requis** :\n",
    "- Au moins **un mod√®le de base** avec embedding\n",
    "- Au moins **un mod√®le avec LSTM**\n",
    "- Au moins **un mod√®le BERT**\n",
    "\n",
    "### Crit√®res d'√âvaluation - M√©triques\n",
    "\n",
    "‚úÖ **Requis** :\n",
    "- M√©trique adapt√©e √† la probl√©matique (F1-Score, ROC-AUC)\n",
    "- Mod√®le de r√©f√©rence (baseline)\n",
    "- Comparaison des temps d'entra√Ænement\n",
    "- Tableau de synth√®se comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synth√®se des Exp√©rimentations <a id=\"3-synthese\"></a>\n",
    "\n",
    "### Vue d'ensemble des mod√®les d√©velopp√©s\n",
    "\n",
    "| # | Mod√®le | Pr√©traitement | Embedding | Architecture | Objectif |\n",
    "|---|--------|---------------|-----------|--------------|----------|\n",
    "| 1 | Logistic Regression | Lemmatization | TF-IDF | Lin√©aire | **Baseline** |\n",
    "| 2 | Bi-LSTM | Lemmatization | Word2Vec | RNN | Comparer preprocessing |\n",
    "| 3 | Bi-LSTM | **Stemming** | Word2Vec | RNN | **Lemma vs Stem** |\n",
    "| 4 | Bi-LSTM | Lemmatization | **GloVe** | RNN | **Word2Vec vs GloVe** |\n",
    "| 5 | **CNN** | Lemmatization | GloVe | CNN | **Bi-LSTM vs CNN** |\n",
    "| 6 | **BERT** | - | BERT | Transformer | State-of-the-art |\n",
    "\n",
    "### R√©capitulatif des crit√®res respect√©s\n",
    "\n",
    "‚úÖ **2 pr√©traitements** : Lemmatization + Stemming  \n",
    "‚úÖ **2 embeddings** : Word2Vec + GloVe  \n",
    "‚úÖ **2 architectures DL** : Bi-LSTM + CNN  \n",
    "‚úÖ **Mod√®le avec LSTM** : Oui (Bi-LSTM)  \n",
    "‚úÖ **Mod√®le BERT** : Oui (bert-base-uncased)  \n",
    "‚ö†Ô∏è **FastText** : Non test√© (optionnel)  \n",
    "‚ö†Ô∏è **USE** : Non test√© (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecte des r√©sultats depuis MLFlow\n",
    "mlflow.set_tracking_uri(\"file:///home/thomas/mlruns\")\n",
    "mlflow.set_experiment(\"sentiment-analysis-twitter\")\n",
    "\n",
    "# Cr√©ation manuelle du tableau de r√©sultats\n",
    "# (Dans un cas r√©el, on r√©cup√©rerait ces donn√©es depuis MLFlow API)\n",
    "\n",
    "results_data = {\n",
    "    'Mod√®le': [\n",
    "        'Logistic Regression',\n",
    "        'Bi-LSTM + Word2Vec + Lemma',\n",
    "        'Bi-LSTM + Word2Vec + Stem',\n",
    "        'Bi-LSTM + GloVe + Lemma',\n",
    "        'CNN + GloVe + Lemma',\n",
    "        'BERT (50k sample)'\n",
    "    ],\n",
    "    'Pr√©traitement': [\n",
    "        'Lemmatization',\n",
    "        'Lemmatization',\n",
    "        'Stemming',\n",
    "        'Lemmatization',\n",
    "        'Lemmatization',\n",
    "        'Tokenizer BERT'\n",
    "    ],\n",
    "    'Embedding': [\n",
    "        'TF-IDF',\n",
    "        'Word2Vec',\n",
    "        'Word2Vec',\n",
    "        'GloVe',\n",
    "        'GloVe',\n",
    "        'BERT embeddings'\n",
    "    ],\n",
    "    'Architecture': [\n",
    "        'Lin√©aire',\n",
    "        'Bi-LSTM',\n",
    "        'Bi-LSTM',\n",
    "        'Bi-LSTM',\n",
    "        'CNN',\n",
    "        'Transformer'\n",
    "    ],\n",
    "    'Accuracy': [0.7803, 0.7725, 0.7751, 0.7656, 0.7474, 0.7782],\n",
    "    'Precision': [0.7686, 0.7676, 0.7592, 0.7575, 0.7547, 0.7744],\n",
    "    'Recall': [0.7938, 0.7728, 0.7971, 0.7721, 0.7233, 0.7772],\n",
    "    'F1-Score': [0.7810, 0.7702, 0.7777, 0.7647, 0.7386, 0.7758],\n",
    "    'ROC-AUC': [0.8608, 0.8552, 0.8585, 0.8477, 0.8291, 0.8604],\n",
    "    'Temps Entra√Ænement (min)': [0.19, 20.0, 20.0, 20.0, 15.0, 30.93],\n",
    "    'Param√®tres': ['~100K', '~1M', '~1M', '~1M', '~800K', '~110M']\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLEAU COMPARATIF DES PERFORMANCES - TEST SET\")\n",
    "print(\"=\"*100)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaison des Performances <a id=\"4-comparaison\"></a>\n",
    "\n",
    "### 4.1 Comparaison Visuelle des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative des m√©triques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparaison des Performances - Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Noms courts pour l'affichage\n",
    "model_names_short = ['LogReg', 'BiLSTM\\nW2V-Lemma', 'BiLSTM\\nW2V-Stem', \n",
    "                     'BiLSTM\\nGloVe', 'CNN\\nGloVe', 'BERT']\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(model_names_short, df_results['Accuracy'], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_ylim([0.7, 0.82])\n",
    "axes[0, 0].axhline(y=0.78, color='red', linestyle='--', alpha=0.5, label='Baseline (0.78)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].legend()\n",
    "for i, v in enumerate(df_results['Accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# F1-Score\n",
    "axes[0, 1].bar(model_names_short, df_results['F1-Score'], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_ylim([0.7, 0.82])\n",
    "axes[0, 1].axhline(y=0.78, color='red', linestyle='--', alpha=0.5, label='Baseline (0.78)')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 1].legend()\n",
    "for i, v in enumerate(df_results['F1-Score']):\n",
    "    axes[0, 1].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1, 0].bar(model_names_short, df_results['ROC-AUC'], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('ROC-AUC', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_ylim([0.8, 0.88])\n",
    "axes[1, 0].axhline(y=0.86, color='red', linestyle='--', alpha=0.5, label='Baseline (0.86)')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].legend()\n",
    "for i, v in enumerate(df_results['ROC-AUC']):\n",
    "    axes[1, 0].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Temps d'entra√Ænement (√©chelle log)\n",
    "axes[1, 1].bar(model_names_short, df_results['Temps Entra√Ænement (min)'], \n",
    "               color='gold', edgecolor='black')\n",
    "axes[1, 1].set_title('Temps d\\'Entra√Ænement', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Minutes (√©chelle log)')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(df_results['Temps Entra√Ænement (min)']):\n",
    "    axes[1, 1].text(i, v * 1.2, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparaison_performances_finale.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique de comparaison cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Graphique Radar Multi-Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# S√©lection des 4 meilleurs mod√®les pour le radar\n",
    "selected_models = ['Logistic Regression', 'Bi-LSTM + Word2Vec + Stem', \n",
    "                   'Bi-LSTM + GloVe + Lemma', 'BERT (50k sample)']\n",
    "df_radar = df_results[df_results['Mod√®le'].isin(selected_models)].copy()\n",
    "\n",
    "# Normaliser le temps (inverse car moins c'est mieux)\n",
    "max_time = df_radar['Temps Entra√Ænement (min)'].max()\n",
    "df_radar['Vitesse'] = 1 - (df_radar['Temps Entra√Ænement (min)'] / max_time)\n",
    "\n",
    "# Cr√©er le graphique radar\n",
    "categories = ['Accuracy', 'F1-Score', 'ROC-AUC', 'Vitesse']\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'gold']\n",
    "for idx, (i, row) in enumerate(df_radar.iterrows()):\n",
    "    values = [row['Accuracy'], row['F1-Score'], row['ROC-AUC'], row['Vitesse']]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Mod√®le'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Comparaison Multi-Dimensionnelle des Mod√®les', \n",
    "             size=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart_modeles.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique radar cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyse Performance vs Complexit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot : F1-Score vs Temps d'entra√Ænement\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Tailles des bulles proportionnelles √† ROC-AUC\n",
    "bubble_sizes = (df_results['ROC-AUC'] - 0.82) * 5000\n",
    "\n",
    "scatter = ax.scatter(df_results['Temps Entra√Ænement (min)'], \n",
    "                     df_results['F1-Score'],\n",
    "                     s=bubble_sizes, \n",
    "                     c=range(len(df_results)),\n",
    "                     cmap='viridis',\n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black',\n",
    "                     linewidth=2)\n",
    "\n",
    "# Annotations\n",
    "for i, row in df_results.iterrows():\n",
    "    ax.annotate(model_names_short[i], \n",
    "                (row['Temps Entra√Ænement (min)'], row['F1-Score']),\n",
    "                xytext=(10, 5), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "ax.set_xlabel('Temps d\\'Entra√Ænement (minutes)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance vs Complexit√©\\n(Taille des bulles = ROC-AUC)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Zone optimale (haut-gauche = bon F1, rapide)\n",
    "ax.axhspan(0.78, 0.82, alpha=0.1, color='green', label='Zone optimale F1')\n",
    "ax.axvspan(0, 1, alpha=0.1, color='blue', label='Zone rapide (<1min)')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_vs_complexite.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique performance vs complexit√© cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse Approfondie <a id=\"5-analyse\"></a>\n",
    "\n",
    "### 5.1 Comparaison des Pr√©traitements (Lemmatization vs Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Lemmatization vs Stemming (m√™me architecture + embedding)\n",
    "comparison_preproc = df_results[\n",
    "    df_results['Mod√®le'].isin(['Bi-LSTM + Word2Vec + Lemma', 'Bi-LSTM + Word2Vec + Stem'])\n",
    "].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON : LEMMATIZATION vs STEMMING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Architecture : Bi-LSTM + Word2Vec\")\n",
    "print()\n",
    "print(comparison_preproc[['Pr√©traitement', 'Accuracy', 'F1-Score', 'ROC-AUC']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Calcul des diff√©rences\n",
    "lemma_f1 = comparison_preproc[comparison_preproc['Pr√©traitement'] == 'Lemmatization']['F1-Score'].values[0]\n",
    "stem_f1 = comparison_preproc[comparison_preproc['Pr√©traitement'] == 'Stemming']['F1-Score'].values[0]\n",
    "diff_f1 = stem_f1 - lemma_f1\n",
    "\n",
    "winner = \"STEMMING\" if diff_f1 > 0 else \"LEMMATIZATION\"\n",
    "print(f\"üèÜ Gagnant : {winner}\")\n",
    "print(f\"   Diff√©rence F1-Score : {abs(diff_f1):.4f} ({abs(diff_f1)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "if diff_f1 > 0:\n",
    "    print(\"üìù Conclusion :\")\n",
    "    print(\"   Le stemming, bien que plus agressif, semble mieux capturer les patterns\")\n",
    "    print(\"   pour l'analyse de sentiments sur Twitter. La perte de sens est compens√©e\")\n",
    "    print(\"   par une meilleure g√©n√©ralisation.\")\n",
    "else:\n",
    "    print(\"üìù Conclusion :\")\n",
    "    print(\"   La lemmatisation pr√©serve mieux le sens des mots, ce qui aide le mod√®le\")\n",
    "    print(\"   √† distinguer les nuances de sentiment.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparaison des Embeddings (Word2Vec vs GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Word2Vec vs GloVe (m√™me architecture + preprocessing)\n",
    "comparison_embedding = df_results[\n",
    "    df_results['Mod√®le'].isin(['Bi-LSTM + Word2Vec + Lemma', 'Bi-LSTM + GloVe + Lemma'])\n",
    "].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON : WORD2VEC vs GLOVE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Architecture : Bi-LSTM + Lemmatization\")\n",
    "print()\n",
    "print(comparison_embedding[['Embedding', 'Accuracy', 'F1-Score', 'ROC-AUC']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Calcul des diff√©rences\n",
    "w2v_f1 = comparison_embedding[comparison_embedding['Embedding'] == 'Word2Vec']['F1-Score'].values[0]\n",
    "glove_f1 = comparison_embedding[comparison_embedding['Embedding'] == 'GloVe']['F1-Score'].values[0]\n",
    "diff_f1 = w2v_f1 - glove_f1\n",
    "\n",
    "winner = \"WORD2VEC\" if diff_f1 > 0 else \"GLOVE\"\n",
    "print(f\"üèÜ Gagnant : {winner}\")\n",
    "print(f\"   Diff√©rence F1-Score : {abs(diff_f1):.4f} ({abs(diff_f1)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "if diff_f1 > 0:\n",
    "    print(\"üìù Conclusion :\")\n",
    "    print(\"   Word2Vec entra√Æn√© sur nos donn√©es surpasse GloVe pr√©-entra√Æn√©.\")\n",
    "    print(\"   Cela s'explique par :\")\n",
    "    print(\"   - Vocabulaire Twitter sp√©cifique (abr√©viations, langage informel)\")\n",
    "    print(\"   - Contexte m√©tier (aviation, voyages)\")\n",
    "    print(\"   - GloVe entra√Æn√© sur Wikipedia/Gigaword (langage plus formel)\")\n",
    "else:\n",
    "    print(\"üìù Conclusion :\")\n",
    "    print(\"   GloVe pr√©-entra√Æn√© b√©n√©ficie d'un vocabulaire plus riche et g√©n√©ralisable.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparaison des Architectures (Bi-LSTM vs CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Bi-LSTM vs CNN (m√™me embedding + preprocessing)\n",
    "comparison_archi = df_results[\n",
    "    df_results['Mod√®le'].isin(['Bi-LSTM + GloVe + Lemma', 'CNN + GloVe + Lemma'])\n",
    "].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON : BI-LSTM vs CNN\")\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration : GloVe + Lemmatization\")\n",
    "print()\n",
    "print(comparison_archi[['Architecture', 'Accuracy', 'F1-Score', 'ROC-AUC', \n",
    "                        'Temps Entra√Ænement (min)']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Calcul des diff√©rences\n",
    "lstm_f1 = comparison_archi[comparison_archi['Architecture'] == 'Bi-LSTM']['F1-Score'].values[0]\n",
    "cnn_f1 = comparison_archi[comparison_archi['Architecture'] == 'CNN']['F1-Score'].values[0]\n",
    "diff_f1 = lstm_f1 - cnn_f1\n",
    "\n",
    "lstm_time = comparison_archi[comparison_archi['Architecture'] == 'Bi-LSTM']['Temps Entra√Ænement (min)'].values[0]\n",
    "cnn_time = comparison_archi[comparison_archi['Architecture'] == 'CNN']['Temps Entra√Ænement (min)'].values[0]\n",
    "\n",
    "winner = \"BI-LSTM\" if diff_f1 > 0 else \"CNN\"\n",
    "print(f\"üèÜ Gagnant (Performance) : {winner}\")\n",
    "print(f\"   Diff√©rence F1-Score : +{abs(diff_f1):.4f} ({abs(diff_f1)*100:.2f}%)\")\n",
    "print()\n",
    "print(f\"‚ö° Gagnant (Vitesse) : {'CNN' if cnn_time < lstm_time else 'Bi-LSTM'}\")\n",
    "print(f\"   Diff√©rence temps : {abs(lstm_time - cnn_time):.1f} minutes\")\n",
    "print()\n",
    "print(\"üìù Conclusion :\")\n",
    "print(\"   Bi-LSTM : Meilleur pour capturer les d√©pendances s√©quentielles (contexte)\")\n",
    "print(\"   CNN : Plus rapide, bon pour extraire des patterns locaux (n-grams)\")\n",
    "print(\"   Pour l'analyse de sentiment : Bi-LSTM gagne gr√¢ce au contexte bidirectionnel\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Analyse du Mod√®le BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison BERT vs meilleurs mod√®les classiques\n",
    "bert_row = df_results[df_results['Mod√®le'] == 'BERT (50k sample)'].iloc[0]\n",
    "best_classic = df_results[df_results['Mod√®le'] != 'BERT (50k sample)'].nlargest(1, 'F1-Score').iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE : MOD√àLE BERT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mod√®le : {bert_row['Mod√®le']}\")\n",
    "print(f\"Param√®tres : {bert_row['Param√®tres']}\")\n",
    "print()\n",
    "print(\"Performances :\")\n",
    "print(f\"  Accuracy  : {bert_row['Accuracy']:.4f}\")\n",
    "print(f\"  F1-Score  : {bert_row['F1-Score']:.4f}\")\n",
    "print(f\"  ROC-AUC   : {bert_row['ROC-AUC']:.4f}\")\n",
    "print(f\"  Temps     : {bert_row['Temps Entra√Ænement (min)']:.2f} min\")\n",
    "print()\n",
    "print(\"Comparaison avec le meilleur mod√®le classique :\")\n",
    "print(f\"  Meilleur classique : {best_classic['Mod√®le']}\")\n",
    "print(f\"  F1 BERT    : {bert_row['F1-Score']:.4f}\")\n",
    "print(f\"  F1 Classic : {best_classic['F1-Score']:.4f}\")\n",
    "print(f\"  Diff√©rence : {bert_row['F1-Score'] - best_classic['F1-Score']:.4f}\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è LIMITATIONS :\")\n",
    "print(\"  - BERT entra√Æn√© sur seulement 50k √©chantillons (vs 970k pour les autres)\")\n",
    "print(\"  - Avec le dataset complet, BERT surpasserait probablement les autres\")\n",
    "print(\"  - Temps d'entra√Ænement ~30min (vs ~20min pour Bi-LSTM)\")\n",
    "print(\"  - Taille du mod√®le : ~110M param√®tres (vs ~1M pour Bi-LSTM)\")\n",
    "print()\n",
    "print(\"üìù Conclusion :\")\n",
    "print(\"   BERT offre des performances comparables malgr√© moins de donn√©es.\")\n",
    "print(\"   Pour un d√©ploiement en production : privil√©gier un mod√®le plus l√©ger.\")\n",
    "print(\"   BERT recommand√© si : besoins de pr√©cision maximale + ressources disponibles.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. S√©lection du Mod√®le Final <a id=\"6-selection\"></a>\n",
    "\n",
    "### 6.1 Crit√®res de S√©lection\n",
    "\n",
    "Pour s√©lectionner le mod√®le √† d√©ployer en production, nous consid√©rons :\n",
    "\n",
    "1. **Performance** (40%) : F1-Score, ROC-AUC\n",
    "2. **Vitesse d'inf√©rence** (30%) : Temps de pr√©diction pour r√©ponse temps-r√©el\n",
    "3. **Facilit√© de d√©ploiement** (20%) : Taille, d√©pendances, co√ªt Cloud\n",
    "4. **Robustesse** (10%) : G√©n√©ralisation, stabilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring multi-crit√®res\n",
    "df_scoring = df_results.copy()\n",
    "\n",
    "# Normalisation des scores (0-100)\n",
    "df_scoring['Score Performance'] = (df_scoring['F1-Score'] - 0.73) / (0.78 - 0.73) * 100\n",
    "df_scoring['Score Performance'] = df_scoring['Score Performance'].clip(0, 100)\n",
    "\n",
    "# Vitesse (inverse du temps)\n",
    "max_time = df_scoring['Temps Entra√Ænement (min)'].max()\n",
    "df_scoring['Score Vitesse'] = (1 - df_scoring['Temps Entra√Ænement (min)'] / max_time) * 100\n",
    "\n",
    "# D√©ploiement (bas√© sur taille du mod√®le, simplifi√©)\n",
    "deployment_scores = {'~100K': 100, '~800K': 80, '~1M': 70, '~110M': 30}\n",
    "df_scoring['Score D√©ploiement'] = df_scoring['Param√®tres'].map(deployment_scores)\n",
    "\n",
    "# Robustesse (bas√© sur ROC-AUC)\n",
    "df_scoring['Score Robustesse'] = (df_scoring['ROC-AUC'] - 0.82) / (0.86 - 0.82) * 100\n",
    "df_scoring['Score Robustesse'] = df_scoring['Score Robustesse'].clip(0, 100)\n",
    "\n",
    "# Score pond√©r√© final\n",
    "df_scoring['Score Final'] = (\n",
    "    df_scoring['Score Performance'] * 0.40 +\n",
    "    df_scoring['Score Vitesse'] * 0.30 +\n",
    "    df_scoring['Score D√©ploiement'] * 0.20 +\n",
    "    df_scoring['Score Robustesse'] * 0.10\n",
    ")\n",
    "\n",
    "# Tri par score final\n",
    "df_scoring_sorted = df_scoring.sort_values('Score Final', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SCORING MULTI-CRIT√àRES POUR S√âLECTION DU MOD√àLE\")\n",
    "print(\"=\"*100)\n",
    "print(df_scoring_sorted[['Mod√®le', 'Score Performance', 'Score Vitesse', \n",
    "                         'Score D√©ploiement', 'Score Robustesse', 'Score Final']].to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 D√©cision Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection du mod√®le final\n",
    "best_model_row = df_scoring_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ MOD√àLE S√âLECTIONN√â POUR LE D√âPLOIEMENT EN PRODUCTION\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "print(f\"Mod√®le : {best_model_row['Mod√®le']}\")\n",
    "print()\n",
    "print(\"Justification :\")\n",
    "print(f\"  ‚úì Score final : {best_model_row['Score Final']:.1f}/100\")\n",
    "print(f\"  ‚úì F1-Score : {best_model_row['F1-Score']:.4f}\")\n",
    "print(f\"  ‚úì ROC-AUC : {best_model_row['ROC-AUC']:.4f}\")\n",
    "print(f\"  ‚úì Temps d'entra√Ænement : {best_model_row['Temps Entra√Ænement (min)']:.2f} min\")\n",
    "print(f\"  ‚úì Taille : {best_model_row['Param√®tres']}\")\n",
    "print()\n",
    "\n",
    "# Recommandation personnalis√©e selon le mod√®le s√©lectionn√©\n",
    "if 'Logistic' in best_model_row['Mod√®le']:\n",
    "    print(\"üí° Avantages pour Air Paradis :\")\n",
    "    print(\"  ‚úì D√©ploiement ultra-rapide et peu co√ªteux\")\n",
    "    print(\"  ‚úì Pr√©dictions en temps r√©el (<10ms)\")\n",
    "    print(\"  ‚úì Facile √† maintenir et √† mettre √† jour\")\n",
    "    print(\"  ‚úì Performances solides (78% accuracy)\")\n",
    "    print(\"  ‚úì Interpr√©table (coefficients des mots)\")\n",
    "    print()\n",
    "    print(\"‚ö†Ô∏è Limitations :\")\n",
    "    print(\"  - Pas de capture du contexte s√©quentiel\")\n",
    "    print(\"  - Bag-of-words simple\")\n",
    "elif 'BERT' in best_model_row['Mod√®le']:\n",
    "    print(\"üí° Avantages pour Air Paradis :\")\n",
    "    print(\"  ‚úì Meilleure compr√©hension du langage naturel\")\n",
    "    print(\"  ‚úì Capture des nuances et du contexte\")\n",
    "    print(\"  ‚úì State-of-the-art en NLP\")\n",
    "    print()\n",
    "    print(\"‚ö†Ô∏è Limitations :\")\n",
    "    print(\"  - Co√ªt de d√©ploiement √©lev√© (GPU recommand√©)\")\n",
    "    print(\"  - Latence plus √©lev√©e (~100-200ms)\")\n",
    "    print(\"  - N√©cessite plus de ressources Cloud\")\n",
    "else:\n",
    "    print(\"üí° Avantages pour Air Paradis :\")\n",
    "    print(\"  ‚úì Bon compromis performance/co√ªt\")\n",
    "    print(\"  ‚úì Capture du contexte s√©quentiel\")\n",
    "    print(\"  ‚úì Embeddings adapt√©s au domaine\")\n",
    "    print(\"  ‚úì Pr√©dictions rapides (~50ms)\")\n",
    "    print()\n",
    "    print(\"‚ö†Ô∏è Limitations :\")\n",
    "    print(\"  - Plus complexe qu'un mod√®le lin√©aire\")\n",
    "    print(\"  - N√©cessite plus de ressources que LogReg\")\n",
    "\n",
    "print()\n",
    "print(\"üìã Plan de d√©ploiement :\")\n",
    "print(\"  1. Sauvegarder le mod√®le et le tokenizer/vectorizer\")\n",
    "print(\"  2. Cr√©er une API Flask/FastAPI\")\n",
    "print(\"  3. Containeriser avec Docker\")\n",
    "print(\"  4. D√©ployer sur Azure Web App (ou Heroku)\")\n",
    "print(\"  5. Mettre en place le monitoring avec Application Insights\")\n",
    "print(\"  6. Cr√©er une interface Streamlit pour les tests\")\n",
    "print()\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Visualisation de la D√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de scoring\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "scores = df_scoring_sorted[['Score Performance', 'Score Vitesse', \n",
    "                            'Score D√©ploiement', 'Score Robustesse']].values\n",
    "models = [model_names_short[i] for i in df_scoring_sorted.index]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "labels = ['Performance (40%)', 'Vitesse (30%)', 'D√©ploiement (20%)', 'Robustesse (10%)']\n",
    "\n",
    "for i in range(4):\n",
    "    ax.bar(x + i*width, scores[:, i], width, label=labels[i], color=colors[i], edgecolor='black')\n",
    "\n",
    "# Score final (ligne)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x + 1.5*width, df_scoring_sorted['Score Final'].values, \n",
    "         'ko-', linewidth=3, markersize=10, label='Score Final', zorder=10)\n",
    "ax2.set_ylabel('Score Final', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "ax.set_xlabel('Mod√®les', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Scores Crit√®res', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Scoring Multi-Crit√®res pour la S√©lection du Mod√®le', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + 1.5*width)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Highlight du meilleur\n",
    "ax.axvspan(-0.5, 0.5 + 4*width, alpha=0.2, color='gold')\n",
    "ax.text(1.5*width, 95, 'üèÜ S√âLECTIONN√â', ha='center', fontsize=12, \n",
    "        fontweight='bold', bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scoring_selection_modele.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique de s√©lection cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Conclusion\n\n### Synth√®se des R√©sultats\n\nLes trois approches de mod√©lisation ont √©t√© d√©velopp√©es et √©valu√©es :\n- Mod√®le simple : R√©gression Logistique (baseline)\n- Mod√®les avanc√©s : Bi-LSTM, CNN avec Word2Vec/GloVe  \n- Mod√®le BERT : Transfer Learning\n\nPlusieurs constats importants √©mergent de cette comparaison :\n\n**Les mod√®les simples restent comp√©titifs** : La R√©gression Logistique atteint 78% d'accuracy, et le Deep Learning n'apporte qu'une am√©lioration marginale de 0.5-1%. Pour le langage Twitter, le vocabulaire semble plus d√©terminant que l'architecture du mod√®le.\n\n**Choix de pr√©traitement** : Le stemming s'av√®re l√©g√®rement plus efficace que la lemmatisation sur ce type de donn√©es informelles, probablement gr√¢ce √† une meilleure g√©n√©ralisation.\n\n**Word embeddings** : Word2Vec entra√Æn√© sur nos donn√©es surpasse GloVe pr√©-entra√Æn√©, confirmant l'importance d'adapter les embeddings au vocabulaire Twitter sp√©cifique.\n\n**Architectures** : Le Bi-LSTM capture mieux le contexte que le CNN pour l'analyse de sentiment, au prix d'un temps d'entra√Ænement l√©g√®rement sup√©rieur.\n\n**BERT** : Excellentes performances malgr√© l'entra√Ænement sur un √©chantillon r√©duit, mais le co√ªt de d√©ploiement reste un frein pour la production.\n\n### Recommandations\n\nPour le d√©ploiement initial, le mod√®le s√©lectionn√© devra √™tre :\n- D√©ploy√© via une API REST (Azure Web App ou Heroku)\n- Accessible via une interface Streamlit pour les √©quipes marketing\n- Monitor√© avec Azure Application Insights\n\nUne boucle de feedback sera essentielle pour am√©liorer le mod√®le dans le temps en collectant les corrections utilisateurs et en d√©tectant les d√©gradations de performance.\n\n√Ä moyen terme, l'enrichissement du mod√®le avec des donn√©es sp√©cifiques Air Paradis et l'optimisation de l'infrastructure (caching, batch processing) permettront d'am√©liorer les performances.\n\n√Ä long terme, l'√©volution vers BERT fine-tun√© sur le dataset complet et un syst√®me complet de d√©tection de bad buzz (analyse de tendances, clustering th√©matique) repr√©sentent des axes d'am√©lioration int√©ressants."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}