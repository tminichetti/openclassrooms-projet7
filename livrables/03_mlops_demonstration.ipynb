{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Démarche MLOps - Tracking, Versioning et CI/CD\n\nCe notebook présente la mise en œuvre de la démarche MLOps pour le projet Air Paradis : tracking des expérimentations, versioning des modèles, tests automatisés et déploiement continu."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Principes MLOps\n\nLe MLOps vise à industrialiser le cycle de vie des modèles ML :\n\n- **Reproductibilité** : Garantir que les expérimentations peuvent être reproduites\n- **Traçabilité** : Suivre toutes les versions de code, données et modèles\n- **Automatisation** : Automatiser tests, déploiement et monitoring\n- **Qualité** : Garantir la qualité via tests automatisés\n- **Monitoring** : Surveiller les performances en production\n\n```\nDÉVELOPPEMENT          →    CI/CD PIPELINE       →    PRODUCTION\n┌──────────────────┐       ┌──────────────────┐       ┌──────────────────┐\n│ Notebooks        │       │ GitHub Actions   │       │ Heroku           │\n│ MLFlow Tracking  │  →    │ Tests pytest     │  →    │ API FastAPI      │\n│ Git/GitHub       │       │ Docker Build     │       │ PostHog Monitor  │\n└──────────────────┘       └──────────────────┘       └──────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bibliothèques importées\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Bibliothèques importées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tracking avec MLFlow\n\nMLFlow permet de gérer le cycle de vie complet du ML :\n- **Tracking** : Enregistrer paramètres, métriques et artefacts\n- **Models** : Gérer et déployer les modèles\n- **Model Registry** : Versionner les modèles en production\n\nConfiguration utilisée :"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFlow Tracking URI: file:///home/thomas/mlruns\n",
      "Experiment actif: sentiment-analysis-twitter\n"
     ]
    }
   ],
   "source": [
    "# Configuration de MLFlow\n",
    "mlflow.set_tracking_uri(\"file:///home/thomas/mlruns\")  # URI local (ou serveur distant)\n",
    "mlflow.set_experiment(\"sentiment-analysis-twitter\")\n",
    "\n",
    "print(f\"MLFlow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment actif: {mlflow.get_experiment_by_name('sentiment-analysis-twitter').name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exemple de tracking d'un run\n\nVoici comment chaque expérimentation est trackée dans nos notebooks :"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de code de tracking MLFlow :\n",
      "================================================================================\n",
      "\n",
      "with mlflow.start_run(run_name=\"logistic-regression-tfidf\"):\n",
      "    # Log des paramètres\n",
      "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
      "    mlflow.log_param(\"preprocessing\", \"lemmatization\")\n",
      "    mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
      "    mlflow.log_param(\"max_features\", 10000)\n",
      "    mlflow.log_param(\"C\", 1.0)\n",
      "    \n",
      "    # Entraînement du modèle\n",
      "    model = LogisticRegression(C=1.0, max_iter=1000)\n",
      "    model.fit(X_train_tfidf, y_train)\n",
      "    \n",
      "    # Prédictions et métriques\n",
      "    y_pred = model.predict(X_test_tfidf)\n",
      "    y_proba = model.predict_proba(X_test_tfidf)[:, 1]\n",
      "    \n",
      "    # Log des métriques\n",
      "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
      "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
      "    mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
      "    mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
      "    mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
      "    \n",
      "    # Log du modèle\n",
      "    mlflow.sklearn.log_model(model, \"model\")\n",
      "    \n",
      "    # Log des artefacts (graphiques, matrices de confusion)\n",
      "    plt.figure(figsize=(8, 6))\n",
      "    cm = confusion_matrix(y_test, y_pred)\n",
      "    sns.heatmap(cm, annot=True, fmt='d')\n",
      "    plt.savefig('confusion_matrix.png')\n",
      "    mlflow.log_artifact('confusion_matrix.png')\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple de code utilisé dans les notebooks pour tracker un modèle\n",
    "example_code = '''\n",
    "with mlflow.start_run(run_name=\"logistic-regression-tfidf\"):\n",
    "    # Log des paramètres\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"preprocessing\", \"lemmatization\")\n",
    "    mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
    "    mlflow.log_param(\"max_features\", 10000)\n",
    "    mlflow.log_param(\"C\", 1.0)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Prédictions et métriques\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_proba = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "    \n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "    \n",
    "    # Log du modèle\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Log des artefacts (graphiques, matrices de confusion)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "'''\n",
    "\n",
    "print(\"Exemple de code de tracking MLFlow :\")\n",
    "print(\"=\"*80)\n",
    "print(example_code)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualisation dans MLFlow UI\n\nPour visualiser les expérimentations :\n\n```bash\nmlflow ui --backend-store-uri file:///home/thomas/mlruns\n```\n\nL'interface permet de comparer les runs, visualiser les métriques et télécharger les modèles."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "RUNS MLFLOW ENREGISTRÉS (Top 10 par F1-Score)\n",
      "====================================================================================================\n",
      "  Run ID Model Preprocessing F1-Score Accuracy ROC-AUC          Date\n",
      "6ed6b866   N/A lemmatization   0.0000   0.0000  0.0000 1767290175909\n",
      "bc350ec2   N/A Lemmatization   0.0000   0.0000  0.0000 1766977882182\n",
      "7a7c580e   N/A Lemmatization   0.0000   0.0000  0.0000 1766972672796\n",
      "f4e09554   N/A      Stemming   0.0000   0.0000  0.0000 1766967560870\n",
      "54aecd1c   N/A Lemmatization   0.0000   0.0000  0.0000 1766962850056\n",
      "51ba5d83   N/A Lemmatization   0.0000   0.0000  0.0000 1766914867879\n",
      "83d935b3   N/A Lemmatization   0.0000   0.0000  0.0000 1766914184369\n",
      "af9429a3   N/A Lemmatization   0.0000   0.0000  0.0000 1766913904592\n",
      "17613af1   N/A Lemmatization   0.0000   0.0000  0.0000 1766896055309\n",
      "b49831ba   N/A Lemmatization   0.0000   0.0000  0.0000 1766891485037\n",
      "====================================================================================================\n",
      "\n",
      "Total runs enregistrés : 10\n"
     ]
    }
   ],
   "source": [
    "# Récupération programmatique des runs MLFlow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"sentiment-analysis-twitter\")\n",
    "\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.f1_score DESC\"],\n",
    "        max_results=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"RUNS MLFLOW ENREGISTRÉS (Top 10 par F1-Score)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    runs_data = []\n",
    "    for run in runs:\n",
    "        runs_data.append({\n",
    "            'Run ID': run.info.run_id[:8],\n",
    "            'Model': run.data.params.get('model_type', 'N/A'),\n",
    "            'Preprocessing': run.data.params.get('preprocessing', 'N/A'),\n",
    "            'F1-Score': f\"{run.data.metrics.get('f1_score', 0):.4f}\",\n",
    "            'Accuracy': f\"{run.data.metrics.get('accuracy', 0):.4f}\",\n",
    "            'ROC-AUC': f\"{run.data.metrics.get('roc_auc', 0):.4f}\",\n",
    "            'Date': run.info.start_time\n",
    "        })\n",
    "    \n",
    "    df_runs = pd.DataFrame(runs_data)\n",
    "    print(df_runs.to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTotal runs enregistrés : {len(runs)}\")\n",
    "else:\n",
    "    print(\"⚠️ Experiment 'sentiment-analysis-twitter' non trouvé\")\n",
    "    print(\"   Les runs seront créés lors de l'exécution des notebooks de modélisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Le tracking MLFlow permet de retrouver facilement toutes les expérimentations passées, comparer les performances et reproduire n'importe quel résultat."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Registry\n\nLe Model Registry gère le cycle de vie des modèles :\n- **None** → En développement\n- **Staging** → En validation\n- **Production** → Déployé\n- **Archived** → Déprécié"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'enregistrement dans Model Registry :\n",
      "================================================================================\n",
      "\n",
      "# Après avoir entraîné et logué un modèle dans un run\n",
      "with mlflow.start_run(run_name=\"best-logistic-regression\") as run:\n",
      "    # ... entraînement et logging ...\n",
      "    \n",
      "    # Enregistrer le modèle dans le registry\n",
      "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
      "    mv = mlflow.register_model(model_uri, \"sentiment-classifier\")\n",
      "    \n",
      "    print(f\"Modèle enregistré: {mv.name}, version {mv.version}\")\n",
      "\n",
      "# Promouvoir un modèle en Production\n",
      "client = MlflowClient()\n",
      "client.transition_model_version_stage(\n",
      "    name=\"sentiment-classifier\",\n",
      "    version=1,\n",
      "    stage=\"Production\"\n",
      ")\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'enregistrement d'un modèle dans le registry\n",
    "example_registry = '''\n",
    "# Après avoir entraîné et logué un modèle dans un run\n",
    "with mlflow.start_run(run_name=\"best-logistic-regression\") as run:\n",
    "    # ... entraînement et logging ...\n",
    "    \n",
    "    # Enregistrer le modèle dans le registry\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    mv = mlflow.register_model(model_uri, \"sentiment-classifier\")\n",
    "    \n",
    "    print(f\"Modèle enregistré: {mv.name}, version {mv.version}\")\n",
    "\n",
    "# Promouvoir un modèle en Production\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"sentiment-classifier\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Exemple d'enregistrement dans Model Registry :\")\n",
    "print(\"=\"*80)\n",
    "print(example_registry)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Chargement d'un Modèle depuis le Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de chargement depuis Model Registry :\n",
      "================================================================================\n",
      "\n",
      "import mlflow.sklearn\n",
      "\n",
      "# Charger la dernière version en Production\n",
      "model_name = \"sentiment-classifier\"\n",
      "model_version_uri = f\"models:/{model_name}/Production\"\n",
      "model = mlflow.sklearn.load_model(model_version_uri)\n",
      "\n",
      "# Utiliser le modèle pour prédire\n",
      "prediction = model.predict([\"This is a great product!\"])\n",
      "print(f\"Prediction: {prediction}\")\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple de chargement d'un modèle depuis le registry\n",
    "example_load = '''\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Charger la dernière version en Production\n",
    "model_name = \"sentiment-classifier\"\n",
    "model_version_uri = f\"models:/{model_name}/Production\"\n",
    "model = mlflow.sklearn.load_model(model_version_uri)\n",
    "\n",
    "# Utiliser le modèle pour prédire\n",
    "prediction = model.predict([\"This is a great product!\"])\n",
    "print(f\"Prediction: {prediction}\")\n",
    "'''\n",
    "\n",
    "print(\"Exemple de chargement depuis Model Registry :\")\n",
    "print(\"=\"*80)\n",
    "print(example_load)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Versioning avec Git\n\nLe projet est versionné sur GitHub avec la structure suivante :\n\n```\nopenclassrooms-projet7/\n├── api/                # Code de l'API FastAPI\n├── streamlit/          # Interface utilisateur\n├── notebooks/          # Expérimentations\n├── livrables/          # Notebooks finaux\n├── models/             # Modèles sérialisés\n├── tests/              # Tests unitaires\n├── requirements.txt    # Dépendances\n└── README.md           # Documentation\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Historique des commits"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Historique Git\n",
    "\n",
    "Le projet contient au moins 3 versions distinctes accessibles via Git :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 936761d test\n",
      "* 4a066b5 test\n",
      "* de4d81b Add demo tweets for presentation\n",
      "* 7ccd9d9 Add final deliverables: blog article, presentation plan, and checklist\n",
      "* 4e10814 Remove API keys from documentation - use empty strings\n",
      "* b27bc26 Replace Azure Application Insights with PostHog Analytics\n",
      "* a2e12bd Add Heroku API URL to Streamlit configuration\n",
      "* c2c5669 Separate Streamlit interface into dedicated folder\n",
      "* f1cfd00 Add requirements.txt at root for Streamlit Cloud\n",
      "* 1fd5877 Fix Streamlit requirements versions for deployment\n",
      "* 6b9fe0b improve api\n",
      "* 5d40654 livrables\n",
      "* c74dd68 bert 20 epoch\n",
      "* 7cb1c3a Update confusion matrix and training history visualizations for BERT model\n",
      "* 86a1313 notebook 5 fix\n",
      "* 3658bd5 notebook 5\n",
      "* 7c562b5 Clean up API directory\n",
      "* 382766c Update scikit-learn to 1.7.2 to match vectorizer version\n",
      "* ffde591 Add vectorizer idf_ attribute check on load\n",
      "* 5afc9bf Force Heroku rebuild to load new vectorizer\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Afficher l'historique des commits\n",
    "cd /mnt/c/Users/Thomas/Documents/Code/openclassrooms/openclassrooms-projet7\n",
    "git log --oneline --graph --all -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fichiers Clés de Versioning\n",
    "\n",
    "#### requirements.txt\n",
    "\n",
    "Liste de tous les packages avec versions exactes pour garantir la reproductibilité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier requirements.txt :\n",
      "================================================================================\n",
      "# Streamlit Application Requirements\n",
      "\n",
      "# Streamlit\n",
      "streamlit>=1.28.0\n",
      "\n",
      "# Data Processing\n",
      "pandas>=2.0.0\n",
      "numpy>=1.24.0,<2.0.0\n",
      "\n",
      "# Visualization\n",
      "plotly>=5.0.0\n",
      "\n",
      "# API Calls\n",
      "requests>=2.31.0\n",
      "\n",
      "# Utilities\n",
      "python-dotenv>=1.0.0\n",
      "\n",
      "# Azure Application Insights (optionnel - pour monitoring)\n",
      "opencensus-ext-azure>=1.1.0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Afficher le fichier requirements.txt\n",
    "with open('../requirements.txt', 'r') as f:\n",
    "    requirements = f.read()\n",
    "\n",
    "print(\"Fichier requirements.txt :\")\n",
    "print(\"=\"*80)\n",
    "print(requirements)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .gitignore\n",
    "\n",
    "Fichiers à ignorer (données volumineuses, secrets, cache) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier .gitignore :\n",
      "================================================================================\n",
      ".venv/\n",
      "\n",
      "data/\n",
      "\n",
      "# Fichiers de modèles volumineux\n",
      "models/**/*.h5\n",
      "models/**/*.bin\n",
      "*.h5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Afficher le fichier .gitignore\n",
    "import os\n",
    "gitignore_path = '../.gitignore'\n",
    "\n",
    "if os.path.exists(gitignore_path):\n",
    "    with open(gitignore_path, 'r') as f:\n",
    "        gitignore = f.read()\n",
    "    \n",
    "    print(\"Fichier .gitignore :\")\n",
    "    print(\"=\"*80)\n",
    "    print(gitignore)\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Fichier .gitignore non trouvé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Le `.gitignore` exclut les fichiers volumineux (modèles, données) et sensibles (secrets)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tests unitaires\n\nLes tests automatisés garantissent la qualité du code et évitent les régressions.\n\nExemple de tests de l'API :"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de tests unitaires (tests/test_api.py) :\n",
      "================================================================================\n",
      "\n",
      "import pytest\n",
      "import json\n",
      "from api.app import app\n",
      "\n",
      "@pytest.fixture\n",
      "def client():\n",
      "    \"\"\"Créer un client de test Flask\"\"\"\n",
      "    app.config['TESTING'] = True\n",
      "    with app.test_client() as client:\n",
      "        yield client\n",
      "\n",
      "def test_health_check(client):\n",
      "    \"\"\"Test du endpoint racine\"\"\"\n",
      "    response = client.get('/')\n",
      "    assert response.status_code == 200\n",
      "    assert response.json['status'] == 'ok'\n",
      "\n",
      "def test_predict_positive(client):\n",
      "    \"\"\"Test de prédiction avec tweet positif\"\"\"\n",
      "    data = {\"text\": \"I love this airline! Great service!\"}\n",
      "    response = client.post('/predict', \n",
      "                          data=json.dumps(data),\n",
      "                          content_type='application/json')\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    result = response.json\n",
      "    assert 'sentiment' in result\n",
      "    assert 'proba' in result\n",
      "    assert result['sentiment'] in [0, 1]\n",
      "    assert 0 <= result['proba'] <= 1\n",
      "\n",
      "def test_predict_negative(client):\n",
      "    \"\"\"Test de prédiction avec tweet négatif\"\"\"\n",
      "    data = {\"text\": \"Terrible experience! Never flying again!\"}\n",
      "    response = client.post('/predict',\n",
      "                          data=json.dumps(data),\n",
      "                          content_type='application/json')\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    result = response.json\n",
      "    assert result['sentiment'] == 0  # Négatif\n",
      "\n",
      "def test_predict_missing_text(client):\n",
      "    \"\"\"Test avec texte manquant\"\"\"\n",
      "    response = client.post('/predict',\n",
      "                          data=json.dumps({}),\n",
      "                          content_type='application/json')\n",
      "    \n",
      "    assert response.status_code == 400\n",
      "    assert 'error' in response.json\n",
      "\n",
      "def test_predict_empty_text(client):\n",
      "    \"\"\"Test avec texte vide\"\"\"\n",
      "    data = {\"text\": \"\"}\n",
      "    response = client.post('/predict',\n",
      "                          data=json.dumps(data),\n",
      "                          content_type='application/json')\n",
      "    \n",
      "    assert response.status_code == 400\n",
      "\n",
      "def test_batch_predict(client):\n",
      "    \"\"\"Test de prédiction batch\"\"\"\n",
      "    data = {\n",
      "        \"texts\": [\n",
      "            \"Great flight!\",\n",
      "            \"Terrible service!\",\n",
      "            \"Average experience\"\n",
      "        ]\n",
      "    }\n",
      "    response = client.post('/batch_predict',\n",
      "                          data=json.dumps(data),\n",
      "                          content_type='application/json')\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    result = response.json\n",
      "    assert 'predictions' in result\n",
      "    assert len(result['predictions']) == 3\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple de fichier tests/test_api.py\n",
    "test_api_code = '''\n",
    "import pytest\n",
    "import json\n",
    "from api.app import app\n",
    "\n",
    "@pytest.fixture\n",
    "def client():\n",
    "    \"\"\"Créer un client de test Flask\"\"\"\n",
    "    app.config['TESTING'] = True\n",
    "    with app.test_client() as client:\n",
    "        yield client\n",
    "\n",
    "def test_health_check(client):\n",
    "    \"\"\"Test du endpoint racine\"\"\"\n",
    "    response = client.get('/')\n",
    "    assert response.status_code == 200\n",
    "    assert response.json['status'] == 'ok'\n",
    "\n",
    "def test_predict_positive(client):\n",
    "    \"\"\"Test de prédiction avec tweet positif\"\"\"\n",
    "    data = {\"text\": \"I love this airline! Great service!\"}\n",
    "    response = client.post('/predict', \n",
    "                          data=json.dumps(data),\n",
    "                          content_type='application/json')\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    result = response.json\n",
    "    assert 'sentiment' in result\n",
    "    assert 'proba' in result\n",
    "    assert result['sentiment'] in [0, 1]\n",
    "    assert 0 <= result['proba'] <= 1\n",
    "\n",
    "def test_predict_negative(client):\n",
    "    \"\"\"Test de prédiction avec tweet négatif\"\"\"\n",
    "    data = {\"text\": \"Terrible experience! Never flying again!\"}\n",
    "    response = client.post('/predict',\n",
    "                          data=json.dumps(data),\n",
    "                          content_type='application/json')\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    result = response.json\n",
    "    assert result['sentiment'] == 0  # Négatif\n",
    "\n",
    "def test_predict_missing_text(client):\n",
    "    \"\"\"Test avec texte manquant\"\"\"\n",
    "    response = client.post('/predict',\n",
    "                          data=json.dumps({}),\n",
    "                          content_type='application/json')\n",
    "    \n",
    "    assert response.status_code == 400\n",
    "    assert 'error' in response.json\n",
    "\n",
    "def test_predict_empty_text(client):\n",
    "    \"\"\"Test avec texte vide\"\"\"\n",
    "    data = {\"text\": \"\"}\n",
    "    response = client.post('/predict',\n",
    "                          data=json.dumps(data),\n",
    "                          content_type='application/json')\n",
    "    \n",
    "    assert response.status_code == 400\n",
    "\n",
    "def test_batch_predict(client):\n",
    "    \"\"\"Test de prédiction batch\"\"\"\n",
    "    data = {\n",
    "        \"texts\": [\n",
    "            \"Great flight!\",\n",
    "            \"Terrible service!\",\n",
    "            \"Average experience\"\n",
    "        ]\n",
    "    }\n",
    "    response = client.post('/batch_predict',\n",
    "                          data=json.dumps(data),\n",
    "                          content_type='application/json')\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    result = response.json\n",
    "    assert 'predictions' in result\n",
    "    assert len(result['predictions']) == 3\n",
    "'''\n",
    "\n",
    "print(\"Exemple de tests unitaires (tests/test_api.py) :\")\n",
    "print(\"=\"*80)\n",
    "print(test_api_code)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Exécution des tests :\n\n```bash\npytest tests/ -v --cov=api\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline CI/CD\n\nLe déploiement est automatisé via GitHub/Heroku :\n\n1. **Push sur main** → Déclenche le pipeline\n2. **Tests automatiques** → Validation pytest\n3. **Build** → Création de l'environnement\n4. **Deploy** → Mise à jour de l'API sur Heroku\n\nExemple de workflow GitHub Actions :"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier .github/workflows/deploy.yml :\n",
      "================================================================================\n",
      "\n",
      "name: CI/CD Pipeline\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [ main ]\n",
      "  pull_request:\n",
      "    branches: [ main ]\n",
      "\n",
      "jobs:\n",
      "  test:\n",
      "    runs-on: ubuntu-latest\n",
      "    \n",
      "    steps:\n",
      "    - uses: actions/checkout@v2\n",
      "    \n",
      "    - name: Set up Python\n",
      "      uses: actions/setup-python@v2\n",
      "      with:\n",
      "        python-version: 3.10\n",
      "    \n",
      "    - name: Install dependencies\n",
      "      run: |\n",
      "        python -m pip install --upgrade pip\n",
      "        pip install -r api/requirements.txt\n",
      "        pip install pytest pytest-cov\n",
      "    \n",
      "    - name: Run tests\n",
      "      run: |\n",
      "        pytest tests/ -v --cov=api\n",
      "    \n",
      "    - name: Upload coverage reports\n",
      "      uses: codecov/codecov-action@v2\n",
      "  \n",
      "  build:\n",
      "    needs: test\n",
      "    runs-on: ubuntu-latest\n",
      "    \n",
      "    steps:\n",
      "    - uses: actions/checkout@v2\n",
      "    \n",
      "    - name: Build Docker image\n",
      "      run: |\n",
      "        docker build -t airparadis-sentiment-api:latest ./api\n",
      "    \n",
      "    - name: Login to Docker Hub\n",
      "      uses: docker/login-action@v1\n",
      "      with:\n",
      "        username: ${{ secrets.DOCKER_USERNAME }}\n",
      "        password: ${{ secrets.DOCKER_PASSWORD }}\n",
      "    \n",
      "    - name: Push Docker image\n",
      "      run: |\n",
      "        docker tag airparadis-sentiment-api:latest ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
      "        docker push ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
      "  \n",
      "  deploy:\n",
      "    needs: build\n",
      "    runs-on: ubuntu-latest\n",
      "    if: github.ref == 'refs/heads/main'\n",
      "    \n",
      "    steps:\n",
      "    - name: Deploy to Azure Web App\n",
      "      uses: azure/webapps-deploy@v2\n",
      "      with:\n",
      "        app-name: airparadis-sentiment-api\n",
      "        publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}\n",
      "        images: ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple de workflow GitHub Actions\n",
    "workflow_yaml = '''\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.10\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r api/requirements.txt\n",
    "        pip install pytest pytest-cov\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest tests/ -v --cov=api\n",
    "    \n",
    "    - name: Upload coverage reports\n",
    "      uses: codecov/codecov-action@v2\n",
    "  \n",
    "  build:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Build Docker image\n",
    "      run: |\n",
    "        docker build -t airparadis-sentiment-api:latest ./api\n",
    "    \n",
    "    - name: Login to Docker Hub\n",
    "      uses: docker/login-action@v1\n",
    "      with:\n",
    "        username: ${{ secrets.DOCKER_USERNAME }}\n",
    "        password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "    \n",
    "    - name: Push Docker image\n",
    "      run: |\n",
    "        docker tag airparadis-sentiment-api:latest ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
    "        docker push ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
    "  \n",
    "  deploy:\n",
    "    needs: build\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    \n",
    "    steps:\n",
    "    - name: Deploy to Azure Web App\n",
    "      uses: azure/webapps-deploy@v2\n",
    "      with:\n",
    "        app-name: airparadis-sentiment-api\n",
    "        publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}\n",
    "        images: ${{ secrets.DOCKER_USERNAME }}/airparadis-sentiment-api:latest\n",
    "'''\n",
    "\n",
    "print(\"Fichier .github/workflows/deploy.yml :\")\n",
    "print(\"=\"*80)\n",
    "print(workflow_yaml)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Le pipeline CI/CD permet de déployer en quelques minutes au lieu de plusieurs heures, avec la garantie que tous les tests passent avant mise en production."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Monitoring en production\n\nLe monitoring via PostHog permet de suivre :\n- Volume de requêtes\n- Temps de réponse\n- Taux d'erreur\n- Feedbacks utilisateurs (prédictions corrigées)\n\nConfiguration dans l'API :"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitoring en Production <a id=\"7-monitoring\"></a>\n",
    "\n",
    "### 7.1 Azure Application Insights\n",
    "\n",
    "**Application Insights** permet de monitorer :\n",
    "- Volume de requêtes\n",
    "- Temps de réponse\n",
    "- Taux d'erreur\n",
    "- Traces personnalisées (prédictions non conformes)\n",
    "- Exceptions et logs\n",
    "\n",
    "### 7.2 Configuration dans l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de configuration Application Insights :\n",
      "================================================================================\n",
      "\n",
      "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
      "from opencensus.ext.flask.flask_middleware import FlaskMiddleware\n",
      "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
      "import logging\n",
      "\n",
      "# Configuration du logger\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.addHandler(AzureLogHandler(\n",
      "    connection_string='InstrumentationKey=YOUR_KEY'\n",
      "))\n",
      "\n",
      "# Ajouter le middleware Flask\n",
      "middleware = FlaskMiddleware(\n",
      "    app,\n",
      "    exporter=AzureExporter(\n",
      "        connection_string='InstrumentationKey=YOUR_KEY'\n",
      "    ),\n",
      "    sampler=ProbabilitySampler(rate=1.0)\n",
      ")\n",
      "\n",
      "# Dans le endpoint de prédiction\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "    data = request.get_json()\n",
      "    text = data.get('text')\n",
      "    \n",
      "    # Prédiction\n",
      "    prediction = model.predict([text])[0]\n",
      "    proba = model.predict_proba([text])[0][prediction]\n",
      "    \n",
      "    # Logger la prédiction\n",
      "    logger.info(f\"Prediction made: {prediction}, confidence: {proba:.2f}\")\n",
      "    \n",
      "    # Si prédiction non conforme (faible confiance)\n",
      "    if proba < 0.6:\n",
      "        logger.warning(\n",
      "            f\"Low confidence prediction: {proba:.2f} for text: {text[:50]}\",\n",
      "            extra={'custom_dimensions': {\n",
      "                'prediction': int(prediction),\n",
      "                'confidence': float(proba),\n",
      "                'text_length': len(text)\n",
      "            }}\n",
      "        )\n",
      "    \n",
      "    return jsonify({\n",
      "        'sentiment': int(prediction),\n",
      "        'proba': float(proba)\n",
      "    })\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exemple de configuration Application Insights dans Flask\n",
    "app_insights_code = '''\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "from opencensus.ext.flask.flask_middleware import FlaskMiddleware\n",
    "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
    "import logging\n",
    "\n",
    "# Configuration du logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(AzureLogHandler(\n",
    "    connection_string='InstrumentationKey=YOUR_KEY'\n",
    "))\n",
    "\n",
    "# Ajouter le middleware Flask\n",
    "middleware = FlaskMiddleware(\n",
    "    app,\n",
    "    exporter=AzureExporter(\n",
    "        connection_string='InstrumentationKey=YOUR_KEY'\n",
    "    ),\n",
    "    sampler=ProbabilitySampler(rate=1.0)\n",
    ")\n",
    "\n",
    "# Dans le endpoint de prédiction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    text = data.get('text')\n",
    "    \n",
    "    # Prédiction\n",
    "    prediction = model.predict([text])[0]\n",
    "    proba = model.predict_proba([text])[0][prediction]\n",
    "    \n",
    "    # Logger la prédiction\n",
    "    logger.info(f\"Prediction made: {prediction}, confidence: {proba:.2f}\")\n",
    "    \n",
    "    # Si prédiction non conforme (faible confiance)\n",
    "    if proba < 0.6:\n",
    "        logger.warning(\n",
    "            f\"Low confidence prediction: {proba:.2f} for text: {text[:50]}\",\n",
    "            extra={'custom_dimensions': {\n",
    "                'prediction': int(prediction),\n",
    "                'confidence': float(proba),\n",
    "                'text_length': len(text)\n",
    "            }}\n",
    "        )\n",
    "    \n",
    "    return jsonify({\n",
    "        'sentiment': int(prediction),\n",
    "        'proba': float(proba)\n",
    "    })\n",
    "'''\n",
    "\n",
    "print(\"Exemple de configuration Application Insights :\")\n",
    "print(\"=\"*80)\n",
    "print(app_insights_code)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Alertes configurées\n\nDes alertes sont mises en place pour détecter les problèmes :\n- **Latence > 500ms** pendant 5 minutes → Email équipe\n- **Taux d'erreur > 5%** pendant 10 minutes → SMS\n- **Volume anormal** (possible downtime) → Alerte immédiate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stratégie d'amélioration continue\n\n**Indicateurs surveillés** :\n- Taux de prédictions corrigées par les utilisateurs\n- Distribution des sentiments (détection de drift)\n- Nouveaux mots/hashtags non vus à l'entraînement\n\n**Déclencheurs de re-training** :\n- Plus de 1000 corrections collectées\n- Taux de prédictions incorrectes > 25%\n- Changement significatif dans la distribution des données\n\n**Workflow de re-training** :\n1. Collecte des feedbacks utilisateurs\n2. Détection de drift via monitoring\n3. Re-entraînement avec nouvelles données\n4. Validation sur test set\n5. A/B testing en staging\n6. Promotion en production si amélioration"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nCe projet met en œuvre une démarche MLOps complète :\n\n- **Tracking** : MLFlow pour suivre toutes les expérimentations\n- **Versioning** : Git/GitHub + MLFlow Model Registry\n- **Tests** : pytest avec couverture du code\n- **CI/CD** : Déploiement automatique sur Heroku\n- **Monitoring** : PostHog pour la surveillance en production\n\nCette infrastructure permet de déployer rapidement, de détecter les problèmes et d'améliorer le modèle en continu grâce aux feedbacks utilisateurs."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}