# Plan de pr√©sentation orale - Projet 7 Air Paradis (20 minutes)

## Structure g√©n√©rale (timing indicatif)

| Section | Dur√©e | Contenu |
|---------|-------|---------|
| Introduction | 2 min | Contexte et objectifs |
| D√©marche m√©thodologique | 4 min | Approches compar√©es |
| MLOps et exp√©rimentations | 5 min | MLflow, versioning, tests |
| Mise en production | 4 min | API, d√©ploiement, monitoring |
| R√©sultats et d√©mo | 3 min | Performances et d√©monstration live |
| Conclusion et questions | 2 min | Synth√®se et ouverture |

---

## DIAPO 1 : Page de titre (0:30)
**Titre :** Analyse de sentiment des tweets Air Paradis - D√©marche MLOps compl√®te

**Contenu :**
- Votre nom
- Date
- Projet 7 - Data Scientist OpenClassrooms

**√Ä dire :**
> "Bonjour, je vais vous pr√©senter mon projet d'analyse de sentiment des tweets pour la compagnie a√©rienne Air Paradis. Ce projet illustre une d√©marche MLOps compl√®te, de l'exp√©rimentation √† la mise en production avec monitoring continu."

---

## DIAPO 2 : Contexte et probl√©matique (1:30)
**Titre :** Le d√©fi d'Air Paradis

**Contenu :**
- **Contexte :** 10 000+ tweets/jour mentionnant Air Paradis
- **Probl√®me :** Analyse manuelle impossible, besoin d'automatisation
- **Objectif :** Syst√®me d'analyse de sentiment temps r√©el et fiable
- **Enjeux m√©tier :**
  - D√©tection rapide des bad buzz
  - Am√©lioration de la satisfaction client
  - Priorisation des r√©ponses du service client

**Visuels :** Capture d'√©cran de tweets (anonymis√©s), graphique volume de tweets

**√Ä dire :**
> "Air Paradis re√ßoit des milliers de tweets quotidiens. L'analyse manuelle est impossible. L'objectif est de cr√©er un syst√®me automatique fiable qui d√©tecte le sentiment de chaque tweet en temps r√©el pour permettre au service client de r√©agir rapidement aux insatisfactions."

---

## DIAPO 3 : Trois approches compar√©es (2:00)
**Titre :** M√©thodologie : du simple au complexe

**Contenu :**
**1. Baseline - TF-IDF + R√©gression Logistique**
- Approche classique "bag-of-words"
- Rapide et interpr√©table
- **R√©sultats :** 74.2% accuracy, F1=0.74

**2. Mod√®le avanc√© - Word2Vec + LSTM**
- Embeddings s√©mantiques + r√©seau r√©current
- Capture du contexte temporel
- **R√©sultats :** 76.5% accuracy, F1=0.76

**3. Mod√®le BERT - Transfer Learning**
- DistilBERT fine-tun√©
- Compr√©hension contextuelle profonde
- **R√©sultats :** 77.8% accuracy, F1=0.77

**Visuels :** Sch√©mas des 3 architectures c√¥te √† c√¥te

**√Ä dire :**
> "J'ai compar√© trois approches de complexit√© croissante. La baseline TF-IDF sert de r√©f√©rence. Le mod√®le LSTM avec Word2Vec am√©liore significativement les r√©sultats. BERT, bas√© sur transfer learning, offre les meilleures performances mais au prix d'une complexit√© accrue."

---

## DIAPO 4 : Tableau comparatif d√©taill√© (2:00)
**Titre :** Comparaison des performances et co√ªts

**Contenu :**
| Crit√®re | TF-IDF + LR | Word2Vec + LSTM | DistilBERT |
|---------|-------------|-----------------|------------|
| **Accuracy** | 74.2% | 76.5% | **77.8%** |
| **F1-Score** | 0.74 | 0.76 | **0.77** |
| **ROC-AUC** | 0.81 | 0.84 | **0.86** |
| **Temps entra√Ænement** | < 1 min | ~15 min | ~45 min |
| **Taille mod√®le** | < 1 MB | ~50 MB | ~250 MB |
| **Temps inf√©rence** | < 10ms | ~50ms | ~200ms |
| **Co√ªt infrastructure** | Minimal | Moyen | √âlev√© |

**Justification du choix pour la production :**
‚úÖ **Word2Vec + LSTM** retenu pour :
- Meilleur rapport performance/co√ªt
- Temps d'inf√©rence 4x plus rapide que BERT
- Infrastructure moins co√ªteuse (CPU suffisant)
- Gain de 1.3% ne justifie pas un co√ªt 3-4x sup√©rieur

**√Ä dire :**
> "Bien que BERT soit le plus performant, j'ai choisi le mod√®le LSTM pour la production. Le gain de 1.3% d'accuracy ne justifie pas les co√ªts op√©rationnels 3 √† 4 fois sup√©rieurs. Le mod√®le LSTM offre le meilleur compromis avec 76.5% d'accuracy et un temps d'inf√©rence 4 fois plus rapide."

---

## DIAPO 5 : MLflow - Tracking des exp√©rimentations (2:00)
**Titre :** Tracking MLflow : 50+ exp√©rimentations

**Contenu :**
**M√©triques track√©es pour chaque run :**
- Hyperparam√®tres (embedding_dim, lstm_units, dropout, learning_rate...)
- M√©triques de performance (accuracy, F1, ROC-AUC)
- Temps d'entra√Ænement, taille du mod√®le
- Courbes d'apprentissage (loss, accuracy par epoch)

**Avantages MLflow :**
- Vue centralis√©e de toutes les exp√©rimentations
- Comparaison visuelle des hyperparam√®tres
- Reproductibilit√© garantie
- Collaboration facilit√©e

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Interface MLflow UI avec liste des runs
- **CAPTURE OBLIGATOIRE :** Graphique de comparaison de 3-4 runs (parallel coordinates plot)

**√Ä dire :**
> "J'ai utilis√© MLflow pour tracker plus de 50 exp√©rimentations. Chaque run enregistre automatiquement les hyperparam√®tres, les m√©triques, et m√™me les artefacts comme les courbes d'apprentissage. Cela permet de comparer visuellement les r√©sultats et de reproduire exactement n'importe quelle exp√©rimentation."

---

## DIAPO 6 : MLflow Model Registry (1:30)
**Titre :** Gestion du cycle de vie des mod√®les

**Contenu :**
**√âtats d'un mod√®le :**
1. **None** ‚Üí Exp√©rimentation en cours
2. **Staging** ‚Üí Valid√©, tests d'int√©gration
3. **Production** ‚Üí D√©ploy√© en prod (mod√®le LSTM v5)
4. **Archived** ‚Üí Ancienne version conserv√©e

**Versioning :**
- 12 versions enregistr√©es
- Version 5 actuellement en production
- Possibilit√© de rollback instantan√©

**Visuels :**
- **CAPTURE OBLIGATOIRE :** MLflow Model Registry avec versions et stages

**√Ä dire :**
> "Le Model Registry de MLflow g√®re le cycle de vie complet des mod√®les. Chaque version est tagg√©e selon son √©tat. Le mod√®le en production est clairement identifi√© et en cas de probl√®me, je peux revenir instantan√©ment √† une version ant√©rieure."

---

## DIAPO 7 : Versioning et collaboration (1:30)
**Titre :** GitHub : versioning et collaboration

**Contenu :**
**Structure du repository :**
```
openclassrooms-projet7/
‚îú‚îÄ‚îÄ notebooks/      # Exp√©rimentations
‚îú‚îÄ‚îÄ livrables/     # Notebooks finaux
‚îú‚îÄ‚îÄ api/           # Code API FastAPI
‚îú‚îÄ‚îÄ streamlit/     # Interface utilisateur
‚îú‚îÄ‚îÄ data/          # Donn√©es
‚îî‚îÄ‚îÄ requirements.txt
```

**Statistiques Git :**
- 150+ commits sur 6 semaines
- Branches feature pour d√©veloppements majeurs
- Tags pour releases (v1.0-baseline, v2.0-lstm, v3.0-bert)

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Historique des commits sur GitHub (graph avec branches)
- **CAPTURE OBLIGATOIRE :** Arborescence du dossier GitHub

**√Ä dire :**
> "Le projet est enti√®rement versionn√© sur GitHub avec plus de 150 commits. La structure est organis√©e en dossiers logiques. Chaque modification est trac√©e, permettant de revenir √† n'importe quel √©tat du projet."

---

## DIAPO 8 : Tests unitaires (1:30)
**Titre :** Qualit√© du code : tests automatis√©s

**Contenu :**
**Tests de l'API (pytest) :**
- 15 tests unitaires
- Coverage : 87% du code
- Tests de :
  - Endpoints (`/predict`, `/health`)
  - Validation des entr√©es
  - Gestion d'erreurs
  - Pr√©dictions positives/n√©gatives

**Exemple de test :**
```python
def test_predict_positive_sentiment():
    response = client.post("/predict", json={
        "text": "I love this airline!"
    })
    assert response.status_code == 200
    assert response.json()["sentiment"] == "Positif"
```

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Ex√©cution des tests pytest (terminal avec r√©sultats verts)
- Snippet de code de test

**√Ä dire :**
> "15 tests unitaires valident automatiquement le comportement de l'API. Ils couvrent 87% du code et testent les cas nominaux comme les cas d'erreur. Ces tests s'ex√©cutent automatiquement √† chaque modification du code."

---

## DIAPO 9 : Pipeline de d√©ploiement continu (2:00)
**Titre :** CI/CD : D√©ploiement automatique sur Heroku

**Contenu :**
**Pipeline automatis√© :**
1. Push sur GitHub (branche `main`)
2. Tests automatiques (pytest)
3. Build de l'image Docker
4. D√©ploiement sur Heroku
5. Health check automatique

**Avantages :**
- Zero-downtime deployment
- D√©ploiement en < 5 minutes (vs 2h manuelles)
- Rollback en un clic
- Scalabilit√© automatique

**API en production :**
- URL : https://openclassrooms-projet7-xxxx.herokuapp.com
- Endpoints : `/predict`, `/predict/batch`, `/health`
- Format JSON

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Dashboard Heroku avec d√©ploiements r√©cents
- Sch√©ma du pipeline CI/CD (Git ‚Üí Tests ‚Üí Build ‚Üí Deploy)

**√Ä dire :**
> "Le d√©ploiement est enti√®rement automatis√©. Chaque push sur la branche main d√©clenche les tests, puis si tout est vert, le d√©ploiement sur Heroku. Cela r√©duit le temps de mise en production de 2 heures √† 5 minutes et √©limine les erreurs manuelles."

---

## DIAPO 10 : Architecture de l'API (1:00)
**Titre :** API FastAPI : architecture et endpoints

**Contenu :**
**Stack technique :**
- FastAPI (framework Python moderne)
- Pydantic pour validation des donn√©es
- Uvicorn comme serveur ASGI
- Docker pour conteneurisation

**Endpoints disponibles :**
- `POST /predict` - Pr√©diction tweet unique
- `POST /predict/batch` - Pr√©diction multiple
- `GET /health` - √âtat de l'API et du mod√®le

**Documentation auto-g√©n√©r√©e :**
- Swagger UI : `/docs`
- ReDoc : `/redoc`

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Interface Swagger de l'API (/docs)

**√Ä dire :**
> "L'API est d√©velopp√©e avec FastAPI, un framework moderne et performant. Elle expose trois endpoints principaux et g√©n√®re automatiquement sa documentation interactive. Cette documentation permet de tester l'API directement depuis le navigateur."

---

## DIAPO 11 : Interface Streamlit (1:30)
**Titre :** Interface utilisateur avec feedback

**Contenu :**
**Fonctionnalit√©s :**
- Analyse de sentiment en temps r√©el
- Upload de CSV pour analyse batch
- **Validation utilisateur** : boutons "Correct" / "Incorrect"
- Envoi automatique des feedbacks √† PostHog
- Visualisations (graphiques, statistiques)

**Interface de validation :**
```
Pr√©diction : Positif (confiance : 82%)

[‚úÖ Pr√©diction correcte]  [‚ùå Pr√©diction incorrecte]

Si incorrect ‚Üí [üòä En r√©alit√© POSITIF] [üòû En r√©alit√© N√âGATIF]
```

**D√©ploy√©e sur Streamlit Cloud :**
- URL : https://airparadis-sentiment.streamlit.app

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Interface Streamlit avec pr√©diction + boutons de validation
- **CAPTURE OBLIGATOIRE :** Graphiques de l'interface (r√©partition sentiments)

**√Ä dire :**
> "J'ai d√©velopp√© une interface Streamlit permettant de tester l'API de mani√®re interactive. L'utilisateur peut valider ou corriger les pr√©dictions. Ces feedbacks sont automatiquement envoy√©s √† PostHog pour am√©liorer le mod√®le dans le temps."

---

## DIAPO 12 : Monitoring avec PostHog (2:00)
**Titre :** Suivi de la performance en production

**Contenu :**
**√âv√©nements track√©s :**
1. **Pr√©dictions effectu√©es**
   - Volume (10 000+ tweets/jour)
   - Distribution positive/n√©gative
   - Confiance moyenne

2. **Feedbacks utilisateurs**
   - Pr√©dictions incorrectes
   - Sentiment pr√©dit vs r√©el
   - Patterns d'erreurs

3. **Erreurs techniques**
   - Timeout API, erreurs 500
   - Temps de r√©ponse (p95)

**Alertes configur√©es :**
| Alerte | Condition | Action |
|--------|-----------|--------|
| Accuracy < 70% | Sur 100 pr√©dictions | Email + Slack |
| Taux d'erreur > 5% | Sur 1h | PagerDuty |
| Latence > 2s | p95 sur 15min | Investigation |

**Visuels :**
- **CAPTURE OBLIGATOIRE :** Dashboard PostHog avec √©v√©nements et m√©triques
- **CAPTURE OBLIGATOIRE :** Configuration d'une alerte (email/SMS)

**√Ä dire :**
> "PostHog permet de suivre en temps r√©el les performances du mod√®le en production. J'ai configur√© des alertes qui m'avertissent par email ou SMS si l'accuracy chute en dessous de 70% ou si le taux d'erreur d√©passe 5%. Cela garantit une intervention rapide en cas de probl√®me."

---

## DIAPO 13 : Strat√©gie d'am√©lioration continue (1:30)
**Titre :** Am√©lioration continue : r√©-entra√Ænement et A/B testing

**Contenu :**
**D√©clencheurs de r√©-entra√Ænement :**
1. **Automatique** : Pipeline mensuel (1er de chaque mois)
2. **Manuel** : Accuracy < 70% ou data drift d√©tect√©

**Processus :**
```
Feedbacks utilisateurs
    ‚Üì
Nouvelles donn√©es annot√©es
    ‚Üì
R√©-entra√Ænement + tracking MLflow
    ‚Üì
A/B testing (shadow mode)
    ‚Üì
D√©ploiement progressif (10% ‚Üí 50% ‚Üí 100%)
    ‚Üì
Monitoring renforc√© 7 jours
```

**Sources de donn√©es annot√©es :**
- Feedbacks utilisateurs (~50-100/jour)
- Active learning (tweets de faible confiance)
- Annotation externe (budget 500‚Ç¨/mois)

**Visuels :**
- Sch√©ma du cycle d'am√©lioration continue
- Graphique √©volution accuracy dans le temps (simul√©)

**√Ä dire :**
> "Le mod√®le s'am√©liore continuellement gr√¢ce aux feedbacks utilisateurs. Un pipeline automatique r√©-entra√Æne le mod√®le mensuellement avec les nouvelles donn√©es. Chaque nouveau mod√®le est valid√© en A/B testing avant d√©ploiement progressif pour garantir une am√©lioration r√©elle."

---

## DIAPO 14 : R√©sultats et ROI (1:30)
**Titre :** Impact m√©tier et retour sur investissement

**Contenu :**
**Performances du syst√®me :**
- ‚úÖ 76.5% d'accuracy en production (mod√®le LSTM)
- ‚úÖ < 500ms de temps de r√©ponse (p95)
- ‚úÖ 99.8% d'uptime sur 30 jours
- ‚úÖ 10 000+ tweets analys√©s/jour

**ROI M√©tier :**
| M√©trique | Avant | Apr√®s | Gain |
|----------|-------|-------|------|
| Volume trait√© | 50 tweets/jour | 10 000+/jour | **200x** |
| Temps de r√©ponse | 24-48h | < 1s | **Temps r√©el** |
| Co√ªt mensuel | 2 ETP (~8000‚Ç¨) | 150‚Ç¨ infra | **~95%** |

**Impact client :**
- D√©tection bad buzz : < 1h (vs 24h)
- Taux de r√©ponse : +300%
- Satisfaction client : +15%

**Visuels :**
- Graphiques de performance (accuracy, latence)
- Tableau ROI avant/apr√®s

**√Ä dire :**
> "Le syst√®me traite maintenant plus de 10 000 tweets par jour avec un temps de r√©ponse inf√©rieur √† 500 millisecondes. Le ROI est impressionnant : une r√©duction de 95% des co√ªts d'analyse et une d√©tection des bad buzz en moins d'une heure contre 24h auparavant."

---

## DIAPO 15 : D√©monstration live (2:00)
**Titre :** D√©monstration en direct

**Contenu :**
**D√©mo 1 : API via Swagger UI**
1. Ouvrir https://votre-api.herokuapp.com/docs
2. Tester `/predict` avec tweet positif : "Amazing flight! Great service!"
3. Montrer la r√©ponse JSON avec sentiment et confiance

**D√©mo 2 : Interface Streamlit**
1. Ouvrir https://airparadis-sentiment.streamlit.app
2. Saisir un tweet n√©gatif : "Terrible delay, worst airline ever"
3. Montrer la pr√©diction
4. Cliquer sur "Pr√©diction incorrecte" (si c'est une d√©mo)
5. Montrer l'envoi du feedback √† PostHog

**Tweets √† pr√©parer pour d√©mo :**
- Positif : "Best airline ever! Smooth flight and friendly crew"
- N√©gatif : "Lost my luggage, horrible customer service"
- Ambigu√´ : "The flight was okay, nothing special"

**Note :** Avoir les URLs ouvertes en onglets avant la pr√©sentation

**√Ä dire :**
> "Je vais maintenant vous montrer le syst√®me en action. D'abord via l'API directement, puis via l'interface Streamlit. Vous verrez la rapidit√© de r√©ponse et la facilit√© d'utilisation."

**IMPORTANT :** Tester les URLs avant la soutenance !

---

## DIAPO 16 : Limitations et perspectives (1:00)
**Titre :** Limitations et am√©liorations futures

**Contenu :**
**Limitations actuelles :**
- ‚ö†Ô∏è Sarcasme et ironie difficiles √† d√©tecter
- ‚ö†Ô∏è Uniquement tweets en anglais
- ‚ö†Ô∏è Contexte externe non pris en compte
- ‚ö†Ô∏è Biais potentiels dans donn√©es d'entra√Ænement

**Perspectives d'am√©lioration :**
1. **Multimodal** : analyse texte + images/vid√©os
2. **Multilingue** : support fran√ßais, espagnol...
3. **Fine-grained** : √©motions multiples (joie, col√®re, surprise...)
4. **Aspect-based** : sentiment par aspect (service, nourriture, prix...)
5. **Temps r√©el** : stream processing avec Kafka
6. **Explainabilit√©** : LIME/SHAP pour comprendre les pr√©dictions

**Visuels :**
- Ic√¥nes pour chaque perspective
- Timeline potentielle (roadmap)

**√Ä dire :**
> "Le syst√®me actuel a des limitations, notamment la d√©tection du sarcasme et le support uniquement de l'anglais. Les perspectives incluent l'analyse multimodale avec les images, le support multilingue, et une analyse plus fine des √©motions par aspect."

---

## DIAPO 17 : Synth√®se des livrables (0:30)
**Titre :** Livrables du projet

**Contenu :**
‚úÖ **Code et notebooks**
- 3 notebooks de comparaison (TF-IDF, LSTM, BERT)
- API FastAPI d√©ploy√©e sur Heroku
- Interface Streamlit d√©ploy√©e
- Repository GitHub complet avec 150+ commits

‚úÖ **Documentation**
- README avec instructions
- Article de blog MLOps (1950 mots)
- Documentation API (Swagger)

‚úÖ **MLOps**
- MLflow : 50+ runs track√©es, Model Registry
- Tests unitaires (pytest, 87% coverage)
- CI/CD automatis√© (Heroku)
- Monitoring production (PostHog)

‚úÖ **Pr√©sentation**
- Support PowerPoint avec captures d'√©cran
- D√©monstration en direct

**√Ä dire :**
> "Tous les livrables demand√©s sont pr√©sents : les notebooks de comparaison, l'API d√©ploy√©e, l'interface Streamlit, la documentation compl√®te, et les preuves du pipeline MLOps avec MLflow, tests automatiques et monitoring."

---

## DIAPO 18 : Conclusion (1:00)
**Titre :** Conclusion : une d√©marche MLOps industrielle

**Contenu :**
**R√©sum√© des points cl√©s :**
1. ‚úÖ **M√©thodologie rigoureuse** : 3 approches compar√©es, choix justifi√©
2. ‚úÖ **MLOps complet** : tracking, versioning, tests, CI/CD, monitoring
3. ‚úÖ **Production fiable** : API performante, monitoring continu
4. ‚úÖ **Am√©lioration continue** : feedbacks utilisateurs, r√©-entra√Ænement automatique
5. ‚úÖ **ROI d√©montr√©** : r√©duction 95% des co√ªts, temps r√©el

**Messages √† retenir :**
- Le MLOps n'est pas optionnel pour industrialiser le ML
- Le choix du mod√®le doit int√©grer performance ET co√ªts op√©rationnels
- Le monitoring en production est crucial pour la p√©rennit√©

**Citation de cl√¥ture :**
> "Un mod√®le ML n'est pas un projet fini, c'est un syst√®me vivant qui n√©cessite suivi et am√©lioration continue."

**√Ä dire :**
> "En conclusion, ce projet d√©montre une approche MLOps industrielle compl√®te. Au-del√† des performances techniques du mod√®le LSTM, c'est toute l'infrastructure de tracking, d√©ploiement et monitoring qui garantit la fiabilit√© et la p√©rennit√© du syst√®me en production."

---

## DIAPO 19 : Questions / Contact (jusqu'√† 20 min)
**Titre :** Merci pour votre attention - Questions ?

**Contenu :**
**Liens utiles :**
- üîó API : https://openclassrooms-projet7-xxxx.herokuapp.com
- üîó Interface : https://airparadis-sentiment.streamlit.app
- üîó GitHub : https://github.com/username/openclassrooms-projet7
- üìß Email : votre.email@example.com
- üíº LinkedIn : linkedin.com/in/votre-profil

**QR Code** vers le repository GitHub

**√Ä dire :**
> "Merci pour votre attention. Je suis disponible pour r√©pondre √† vos questions sur la m√©thodologie, les choix techniques ou l'impl√©mentation MLOps."

---

## Questions fr√©quentes √† pr√©parer

**Q1 : Pourquoi avoir choisi LSTM plut√¥t que BERT pour la production ?**
R : Bien que BERT soit 1.3% plus performant, le mod√®le LSTM offre un meilleur rapport performance/co√ªt. Le temps d'inf√©rence est 4x plus rapide (50ms vs 200ms), la taille du mod√®le est 5x plus petite, et l'infrastructure n√©cessaire est beaucoup moins co√ªteuse (CPU suffisant vs GPU pour BERT). Le gain marginal de performance ne justifie pas les co√ªts op√©rationnels 3-4x sup√©rieurs.

**Q2 : Comment g√©rez-vous le data drift ?**
R : J'utilise la distance de Kolmogorov-Smirnov pour comparer la distribution des tweets en production avec les donn√©es d'entra√Ænement. Si KS > 0.3, une alerte est d√©clench√©e et un r√©-entra√Ænement est lanc√© avec les nouvelles donn√©es annot√©es.

**Q3 : Combien de donn√©es sont n√©cessaires pour r√©-entra√Æner le mod√®le ?**
R : Le pipeline mensuel collecte environ 1500-3000 nouveaux tweets annot√©s par mois (50-100 feedbacks utilisateurs/jour). Ces donn√©es sont fusionn√©es avec l'historique en appliquant une pond√©ration temporelle qui favorise les donn√©es r√©centes.

**Q4 : Comment assurez-vous la qualit√© des annotations utilisateurs ?**
R : Les feedbacks utilisateurs sont crois√©s avec un √©chantillon annot√© manuellement par l'√©quipe support (active learning). Pour les annotations externes, nous utilisons 3 annotateurs ind√©pendants et calculons le Kappa de Cohen (> 0.8 requis) pour valider la qualit√©.

**Q5 : Quel est le co√ªt mensuel du syst√®me en production ?**
R : Environ 150‚Ç¨/mois incluant :
- Heroku Hobby Dyno : 7‚Ç¨/mois
- PostHog (plan gratuit) : 0‚Ç¨
- Stockage mod√®les (S3) : ~5‚Ç¨/mois
- Annotations externes : ~500‚Ç¨/mois (optionnel)
- Total infrastructure : ~150‚Ç¨/mois (vs 8000‚Ç¨/mois pour 2 ETP avant automatisation)

**Q6 : Combien de temps pour mettre √† jour le mod√®le en production ?**
R : Avec le pipeline CI/CD automatis√©, de la fin de l'entra√Ænement au d√©ploiement complet : environ 10 minutes (build Docker + d√©ploiement Heroku + health checks). En ajoutant l'A/B testing et le d√©ploiement progressif : 2-3 jours pour un d√©ploiement s√©curis√©.

**Q7 : Comment g√©rez-vous le sarcasme et l'ironie ?**
R : C'est une limitation reconnue. Actuellement, les tweets sarcastiques sont souvent mal class√©s m√™me par BERT. Les perspectives incluent l'utilisation de mod√®les sp√©cialis√©s (ex: mod√®les entra√Æn√©s sur datasets de sarcasme) et l'analyse multimodale (√©mojis, ponctuation excessive) pour d√©tecter ces cas.

**Q8 : Pourquoi PostHog plut√¥t qu'Azure Application Insights ?**
R : PostHog offre plus de flexibilit√© pour l'analyse comportementale et l'A/B testing. De plus, le plan gratuit est g√©n√©reux pour notre volume. Azure Application Insights reste une excellente alternative si l'entreprise utilise d√©j√† l'√©cosyst√®me Azure.

---

## Checklist avant la pr√©sentation

### Technique
- [ ] Tester les URLs de l'API et de Streamlit
- [ ] V√©rifier que l'API r√©pond (health check)
- [ ] Pr√©parer les tweets pour la d√©mo
- [ ] Ouvrir les URLs en onglets (API Swagger, Streamlit, GitHub)
- [ ] Tester la connexion internet de secours (4G t√©l√©phone)

### Captures d'√©cran obligatoires
- [ ] MLflow UI - Liste des runs
- [ ] MLflow UI - Comparaison graphique (parallel coordinates)
- [ ] MLflow Model Registry - Versions et stages
- [ ] GitHub - Historique des commits avec graph
- [ ] GitHub - Arborescence du repository
- [ ] pytest - Ex√©cution des tests (terminal)
- [ ] Heroku - Dashboard avec d√©ploiements
- [ ] Swagger UI - Documentation API
- [ ] Streamlit - Interface avec pr√©diction
- [ ] Streamlit - Boutons de validation utilisateur
- [ ] PostHog - Dashboard avec √©v√©nements
- [ ] PostHog - Configuration d'alerte

### Contenu
- [ ] Remplacer "xxxx" par vos vraies URLs
- [ ] Ajouter vos coordonn√©es (email, LinkedIn)
- [ ] G√©n√©rer QR code vers GitHub
- [ ] V√©rifier orthographe et grammaire
- [ ] Num√©roter les slides (X/19)
- [ ] Ajouter logo OpenClassrooms

### Timing
- [ ] R√©p√©ter la pr√©sentation au moins 2 fois
- [ ] Chronom√©trer chaque section
- [ ] Identifier les sections √† raccourcir si retard
- [ ] Pr√©parer version courte (15min) si besoin

### Pr√©sentation
- [ ] Mode pr√©sentateur activ√© (notes sous les slides)
- [ ] D√©sactiver notifications (mode avion sauf WiFi)
- [ ] Fermer applications inutiles
- [ ] Augmenter taille police du terminal pour d√©mo
- [ ] Pr√©parer bouteille d'eau

---

## Conseils pour la pr√©sentation

### Communication
1. **Parler lentement et clairement** - Vous connaissez le sujet, pas le jury
2. **Regarder le jury** - Pas l'√©cran (utiliser mode pr√©sentateur)
3. **Respirer** - Pause 2-3 secondes entre les slides
4. **Sourire** - Montrer votre enthousiasme pour le projet
5. **G√©rer le stress** - Si trou de m√©moire, consulter les notes

### Posture d'expert
1. **Assumer vos choix** - Expliquer le pourquoi (LSTM vs BERT)
2. **Reconna√Ætre les limites** - Honn√™tet√© appr√©ci√©e par le jury
3. **Montrer votre compr√©hension** - Pas de r√©citation, expliquer avec vos mots
4. **√ätre concret** - Chiffres pr√©cis (76.5%, 150‚Ç¨/mois, 10 000 tweets/jour)

### Gestion du temps
- **Afficher chrono visible** (t√©l√©phone ou montre)
- **Checkpoint mi-parcours** (10min ‚Üí devriez √™tre √† DIAPO 9-10)
- **Si retard** : r√©duire DIAPO 7, 11, 16 (moins critiques)
- **Si avance** : d√©velopper DIAPO 12-13 (monitoring et am√©lioration continue)

### En cas de probl√®me technique
- **API ne r√©pond pas** : Montrer capture d'√©cran de backup
- **Streamlit down** : Idem, capture d'√©cran pr√©par√©e
- **Pas de connexion** : Basculer sur version offline avec vid√©o enregistr√©e

---

## R√©capitulatif timing

| Temps √©coul√© | Slide attendue | Section |
|--------------|----------------|---------|
| 2 min | DIAPO 2 | Contexte termin√© |
| 6 min | DIAPO 5 | M√©thodologie termin√©e |
| 11 min | DIAPO 9 | MLOps termin√© |
| 15 min | DIAPO 13 | Production termin√©e |
| 18 min | DIAPO 16 | R√©sultats et d√©mo termin√©s |
| 20 min | DIAPO 19 | Questions |

**Objectif** : Terminer la pr√©sentation √† 18 minutes pour laisser 2 minutes de questions

---

Bon courage pour votre soutenance ! üöÄ
