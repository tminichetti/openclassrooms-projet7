Étape 0 : Préparation de l’environnement d’expérimentation MLFlow
Livrables :
Environnement MLFlow permettant le tracking lors de l’entraînement des modèles, la visualisation et la comparaison via l’UI de MLFlow, ainsi que le stockage de manière centralisée des modèles
Fonction de tracking de logs
Recommandations :
L’étudiant a le choix lors de l’installation de définir le mode de stockage des données de tracking, dans des répertoires ou dans une base de données (CF ressource vers le site de référence MLFlow)
Réaliser dès le départ une analyse des données de tracking et stockage à gérer avec MLFlow : scores, autres mesures, paramètres, artifacts, modèles, …
Créer une fonction qui standardise le tracking de données et qui sera appelée lors de chaque entraînement de modèles
Il est attendu au minimum le tracking des scores (accuracy, AUC), des hyperparamètres, des temps de traitement de fit et de prédiction, ainsi que le stockage de graphiques (ROC curve, History plot du fit TensorFlow-Keras) et des modèles
Il est attendu un affichage des résultats via MLFlow UI

#Étape 1 : “Modèle sur mesure simple”
Livrable :
Notebook d’élaboration du modèle classique
Recommandations :
Réaliser un test sur le même sample du dataset que les modèles avancés, par exemple 16000 tweets, afin de comparer les résultats :
Découper en 3 sets : Train, Validation, Test
Le set Train est utilisé pour l’entraînement, le set de Validation pour mesurer les scores du modèle et comparer avec les autres modèles, le set de Test pour réaliser un contrôle final, notamment s’assurer d’un score similaire sur ce set qui n’est à aucun moment intervenu dans l’élaboration ou le choix du modèle
Sur ce modèle classique, l’étudiant pourra réaliser une Cross-Validation sur le set Train. Par contre la comparaison avec les modèles avancés se fera sur le score du set Validation (score du set de Validation utilisé en deep learning pour élaborer les modèles)

#Étape 2 : “Modèle avancé” – Élaboration des modèles
Livrables :
Création de deux modèles de Deep Learning, dont au moins un avec un layer LSTM.
Simulation selon deux techniques de pré-traitement (lemmatization, stemming) sur l’un des 2 modèles, afin de choisir la technique pour la suite des simulations.
Simulation selon 2 approches de word embedding (parmi Word2VEc, Glove, FastText), entraînés avec le jeu de données ou pré-entraînés sur au moins un des 2 modèles de Deep Learning, afin de choisir l’embedding pour la suite des simulations.
Création ensuite d’un modèle BERT, il y a 2 approches possibles :
Générer des features (sentence embedding) à partir d’un TFBertModel (Hugging Face) ou d’un d’un model via le Hub TensorFlow, puis ajouter une ou des couches de classification
Utiliser directement un modèle Hugging Face de type TFBertForSequenceClassification
En option tester USE (Universal Sentence Encoding) pour le feature engineering
Problèmes et erreurs courants :
Temps de traitement et limitation de ressources en TensorFlow-Keras.
Recommandations :
S’inspirer des exemples de modèles cités en ressources
Pour le modèle avec couche LSTM, l’utilisation d’un « Bidirectional (LSTM) » peut permettre de meilleurs résultats.
Réaliser dans un premier temps des tests sur le même sample du dataset que l’approche classique (par exemple 16000 tweets), afin de comparer les résultats avec l’autre approche.
Réaliser ensuite une simulation sur un jeu de données plus important sur le modèle choisi, en fonction des capacités machine à disposition.

#Étape 3 : Suivi de la performance du modèle en production
Livrable :
Traces et alertes sur Azure Application Insight
Blog partie “suivi de la performance du modèle en production” : présentation des traces et alertes sur Azure Application Insight, ainsi qu’une présentation d’une démarche qui pourrait être mise en oeuvre pour l’analyse de ces statistiques et l’amélioration du modèle dans le temps
Recommandations :
L’étudiant crée un service Application Insight sur Azure
Dans son interface de test de l’API (CF milestone suivant) il demande une validation à l’utilisateur de la pertinence de la prédiction, et envoie une “trace” au service Azure Application Insight en cas de non validation
Il paramètre la gestion d’une alerte de son service Azure Application insight, pour déclencher une alerte, par exemple si 3 “traces” sont transmises sur un intervalle de 5 mn
Il peut implémenter optionnellement la remontée également de “traces” de validation par l’utilisateur de la prédiction, ce qui permettra de calculer une accuracy en production, et mesurer une éventuelle dérive des prédictions.
Cette action, implémentée ou pas, fera partie du dispositif de “suivi de la performance du modèle en production” qui sera proposé et décrit dans le blog

#Étape 4 : “Modèle avancé” – Mise en production
Livrables :
Pipeline de mise en production : code de l’API, géré en version en local avec GIT, commits et push du code sur Github, mise en production sur Cloud via les “actions” de Github : exécution d’un fichier yml de “build and deploy” de l’API, intégrant l’exécution automatique de tests unitaires lors du build
Mise en production du modèle pour prédiction du sentiment (API - moteur d’inférence) : l’API renvoie un sentiment à réception d’un texte brut
Interface en local (streamlit, notebook, application web) d’appel à l’API via son uri : interface de saisie du texte brut par l’utilisateur, appel de l’API, réception et affichage du sentiment, demande de validation à l’utilisateur de la pertinence de la prédiction, et envoie d’une trace au service Azure Application Insight en cas de non validation (optionnellement : également en cas de validation)
Recommandations :
Les tests unitaires seront testés en local, puis exécutés automatiquement dans la partie “build” de déploiement via Github : pour cela il est nécessaire de rajouter dans le fichier .yml, dans la partie “deploy”, après l’installation des dépendances, 2 lignes “name: Run unit tests”, puis “run: python -m unittest” (ou pytest, si pytest est utilisé)
Penser à sauvegarder l’ensemble du modèle Tensorflow via un « model.save([nom_répertoire]) » et non pas un fichier « .h5 »
Si la solution Cloud gratuite choisie par l’étudiant ne permet pas de prendre en compte le modèle avancé Tensorflow-(composants, limite de taille, …), l’étudiant pourra déployer son modèle classique.  
Une autre solution est de réduire la taille du modèle TensorFlow-Keras en testant la solution de conversion en un modèle “TensorFlow Lite” : https://www.tensorflow.org/lite/convert/index?hl=fr
Pour tester l’API de prédiction créée, une interface à l’aide de Streamlit est très rapide à mettre en œuvre (exécution en local).
L’étudiant a le choix de la solution Cloud pour le déploiement de l’API, idéalement gratuite :
Azure webapp (App Service)
Choisir l’option de l’App Service Plan avec « Sku and size » égal idéalement à « Free F1 » gratuit (code + packages < 1 Go)
Création sur le portail Azure d’une webapp, avec mise à jour du code automatiquement via une source Github
PythonAnywhere
Heroku, gratuit uniquement en mode “github student” (CF cadre bleu ci-dessus)
Autres solutions au choix
L’étudiant a la possibilité d’optimiser son déploiement en créant un conteneur Docker : https://openclassrooms.com/fr/courses/2035766-optimisez-votre-deploiement-en-creant-des-conteneurs-avec-docker

