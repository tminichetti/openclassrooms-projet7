{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle BERT pour la Classification de Sentiments\n",
    "\n",
    "Ce notebook implémente un modèle BERT (Bidirectional Encoder Representations from Transformers) pour la classification de sentiments des tweets.\n",
    "\n",
    "**Objectif**: Fine-tuner un modèle BERT pré-entraîné sur notre dataset de tweets pour la classification binaire (positif/négatif).\n",
    "\n",
    "**Approche**: Utilisation de `TFBertForSequenceClassification` de Hugging Face avec tokenisation BERT et préparation des input_ids et attention_masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration MLFlow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "mlflow.set_experiment(\"air-paradis-sentiment-analysis\")\n",
    "\n",
    "def log_metrics_to_mlflow(model_name, metrics_dict, model=None, training_time=None, prediction_time=None):\n",
    "    \"\"\"\n",
    "    Fonction pour logger les métriques dans MLFlow de manière standardisée.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nom du modèle\n",
    "        metrics_dict: Dictionnaire contenant les métriques\n",
    "        model: Modèle TensorFlow (optionnel)\n",
    "        training_time: Temps d'entraînement en secondes\n",
    "        prediction_time: Temps de prédiction en secondes\n",
    "    \"\"\"\n",
    "    # Log des métriques\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Log des temps\n",
    "    if training_time:\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    if prediction_time:\n",
    "        mlflow.log_metric(\"prediction_time_seconds\", prediction_time)\n",
    "    \n",
    "    # Log du modèle\n",
    "    if model:\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"✓ Métriques loggées dans MLFlow pour {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des Données\n",
    "\n",
    "Pour BERT, nous utilisons les données **brutes** (non prétraitées) car BERT a son propre tokenizer qui gère la tokenisation, la casse, et les caractères spéciaux de manière optimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chargement des données originales (avant preprocessing NLTK)\n# BERT fonctionne mieux avec le texte brut car il a son propre tokenizer\nprint(\"Chargement des données...\")\n\n# Pour l'entraînement BERT, nous allons utiliser un subset plus petit pour des raisons de temps\n# BERT est très coûteux en ressources\nSAMPLE_SIZE = 50000  # 50k tweets par set (ajustable selon vos ressources)\n\n# Charger les données originales\ntrain_df = pd.read_csv('../data/processed/train_lemmatized.csv').sample(n=SAMPLE_SIZE, random_state=42)\nval_df = pd.read_csv('../data/processed/val_lemmatized.csv').sample(n=int(SAMPLE_SIZE * 0.2), random_state=42)\ntest_df = pd.read_csv('../data/processed/test_lemmatized.csv').sample(n=int(SAMPLE_SIZE * 0.2), random_state=42)\n\n# Pour BERT, nous utilisons le texte (même prétraité, BERT s'en sortira bien)\n# Note: Idéalement, on utiliserait le texte complètement brut, mais le texte lemmatisé fonctionne aussi\n\nprint(f\"Train: {len(train_df)} tweets\")\nprint(f\"Validation: {len(val_df)} tweets\")\nprint(f\"Test: {len(test_df)} tweets\")\n\n# Vérification\nprint(\"\\nExemple de tweet:\")\nprint(train_df['text'].iloc[0])\nprint(f\"\\nSentiment: {train_df['sentiment'].iloc[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenisation BERT\n",
    "\n",
    "BERT nécessite une tokenisation spécifique qui:\n",
    "- Convertit le texte en tokens BERT (WordPiece)\n",
    "- Ajoute les tokens spéciaux [CLS] et [SEP]\n",
    "- Crée les input_ids (IDs des tokens)\n",
    "- Crée les attention_masks (masque pour indiquer les vrais tokens vs padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialisation du tokenizer BERT\nprint(\"Initialisation du tokenizer BERT...\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Configuration de la longueur maximale des séquences\nMAX_LENGTH = 128  # Longueur maximale des tweets (ajustable)\n\ndef tokenize_data(texts, labels, max_length=MAX_LENGTH):\n    \"\"\"\n    Tokenise les textes avec le tokenizer BERT.\n    \n    Returns:\n        input_ids: IDs des tokens\n        attention_masks: Masques d'attention\n        labels: Labels\n    \"\"\"\n    encodings = tokenizer(\n        texts.tolist(),\n        truncation=True,\n        padding='max_length',\n        max_length=max_length,\n        return_tensors='tf'\n    )\n    \n    return encodings['input_ids'], encodings['attention_mask'], tf.constant(labels.values)\n\nprint(\"Tokenisation des données...\")\nstart_time = time.time()\n\n# Tokenisation\ntrain_input_ids, train_attention_masks, train_labels = tokenize_data(train_df['text'], train_df['sentiment'])\nval_input_ids, val_attention_masks, val_labels = tokenize_data(val_df['text'], val_df['sentiment'])\ntest_input_ids, test_attention_masks, test_labels = tokenize_data(test_df['text'], test_df['sentiment'])\n\ntokenization_time = time.time() - start_time\nprint(f\"✓ Tokenisation terminée en {tokenization_time:.2f}s\")\n\n# Vérification des dimensions\nprint(f\"\\nDimensions:\")\nprint(f\"Train input_ids: {train_input_ids.shape}\")\nprint(f\"Train attention_masks: {train_attention_masks.shape}\")\nprint(f\"Train labels: {train_labels.shape}\")\n\n# Exemple de tokenisation\nprint(f\"\\nExemple de tokenisation:\")\nprint(f\"Texte original: {train_df['text'].iloc[0]}\")\nprint(f\"Input IDs: {train_input_ids[0][:20]}...\")  # Premiers 20 tokens\nprint(f\"Attention mask: {train_attention_masks[0][:20]}...\")  # Premiers 20 masques"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création des Datasets TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # Batch size pour BERT (ajustable selon la mémoire GPU)\n",
    "\n",
    "# Création des datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': train_input_ids,\n",
    "    'attention_mask': train_attention_masks\n",
    "}, train_labels))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': val_input_ids,\n",
    "    'attention_mask': val_attention_masks\n",
    "}, val_labels))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': test_input_ids,\n",
    "    'attention_mask': test_attention_masks\n",
    "}, test_labels))\n",
    "\n",
    "# Configuration des datasets pour l'entraînement\n",
    "train_dataset = train_dataset.shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"✓ Datasets créés avec batch_size={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construction du Modèle BERT\n",
    "\n",
    "Nous utilisons `TFBertForSequenceClassification` qui est un modèle BERT pré-entraîné avec une couche de classification ajoutée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_bert_model():\n    \"\"\"\n    Construit un modèle BERT pour la classification binaire.\n    \"\"\"\n    from transformers import create_optimizer\n    \n    model = TFBertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=2,  # Classification binaire\n        use_safetensors=False  # Évite les problèmes de compatibilité\n    )\n    \n    # Note: Pas besoin de compiler manuellement pour BERT\n    # Le modèle est déjà configuré pour l'entraînement\n    \n    return model\n\nprint(\"Construction du modèle BERT...\")\nbert_model = build_bert_model()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ARCHITECTURE DU MODÈLE BERT\")\nprint(\"=\"*80)\nprint(f\"✓ Modèle BERT chargé: bert-base-uncased\")\nprint(f\"✓ Nombre de labels: 2 (classification binaire)\")\nprint(f\"✓ Paramètres: ~110M\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration de l'entraînement\nEPOCHS = 3  # 3-4 epochs sont généralement suffisants pour BERT\nLEARNING_RATE = 2e-5\n\n# Création de l'optimizer et compilation\nfrom transformers import create_optimizer\n\n# Calculer le nombre de steps\nnum_train_steps = len(train_dataset) * EPOCHS\n\noptimizer, lr_schedule = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=int(num_train_steps * 0.1),  # 10% de warmup\n)\n\n# Compiler le modèle avec le bon optimizer\nbert_model.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Début du run MLFlow\nwith mlflow.start_run(run_name=\"BERT-base-uncased\"):\n    # Log des hyperparamètres\n    mlflow.log_param(\"model_type\", \"BERT\")\n    mlflow.log_param(\"pretrained_model\", \"bert-base-uncased\")\n    mlflow.log_param(\"max_length\", MAX_LENGTH)\n    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n    mlflow.log_param(\"epochs\", EPOCHS)\n    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n    mlflow.log_param(\"train_samples\", len(train_df))\n    mlflow.log_param(\"val_samples\", len(val_df))\n    \n    print(\"\\nDébut de l'entraînement...\")\n    print(\"=\"*80)\n    print(\"Note: Entraînement de 3 epochs sans early stopping\")\n    print(\"=\"*80)\n    \n    start_time = time.time()\n    \n    # Entraînement (sans callbacks pour éviter les problèmes de compatibilité)\n    history = bert_model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=EPOCHS,\n        verbose=1\n    )\n    \n    training_time = time.time() - start_time\n    \n    print(\"=\"*80)\n    print(f\"✓ Entraînement terminé en {training_time:.2f}s ({training_time/60:.2f} minutes)\")\n    \n    # Log du temps d'entraînement\n    mlflow.log_metric(\"training_time_seconds\", training_time)\n    mlflow.log_metric(\"training_time_minutes\", training_time/60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation de l'Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique d'entraînement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "axes[0].set_title('Evolution de la Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "axes[1].set_title('Evolution de l\\'Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log dans MLFlow\n",
    "with mlflow.start_run(run_id=mlflow.active_run().info.run_id):\n",
    "    mlflow.log_artifact('bert_training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Évaluation sur le Set de Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Évaluation sur le set de validation...\")\n",
    "\n",
    "# Prédictions\n",
    "start_time = time.time()\n",
    "val_predictions = bert_model.predict(val_dataset)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Extraction des logits et conversion en classes\n",
    "val_logits = val_predictions.logits\n",
    "val_pred_classes = np.argmax(val_logits, axis=1)\n",
    "\n",
    "# Probabilités (pour ROC-AUC)\n",
    "val_probabilities = tf.nn.softmax(val_logits, axis=1).numpy()[:, 1]\n",
    "\n",
    "# Calcul des métriques\n",
    "val_metrics = {\n",
    "    'val_accuracy': accuracy_score(val_labels.numpy(), val_pred_classes),\n",
    "    'val_precision': precision_score(val_labels.numpy(), val_pred_classes),\n",
    "    'val_recall': recall_score(val_labels.numpy(), val_pred_classes),\n",
    "    'val_f1_score': f1_score(val_labels.numpy(), val_pred_classes),\n",
    "    'val_roc_auc': roc_auc_score(val_labels.numpy(), val_probabilities)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSULTATS SUR LE SET DE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy:  {val_metrics['val_accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics['val_precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['val_recall']:.4f}\")\n",
    "print(f\"F1-Score:  {val_metrics['val_f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC:   {val_metrics['val_roc_auc']:.4f}\")\n",
    "print(f\"\\nTemps de prédiction: {prediction_time:.2f}s\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log dans MLFlow\n",
    "with mlflow.start_run(run_id=mlflow.active_run().info.run_id):\n",
    "    log_metrics_to_mlflow(\n",
    "        \"BERT\",\n",
    "        val_metrics,\n",
    "        prediction_time=prediction_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(val_labels.numpy(), val_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Matrice de Confusion - BERT', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Prédite')\n",
    "plt.xticks([0.5, 1.5], ['Négatif (0)', 'Positif (1)'])\n",
    "plt.yticks([0.5, 1.5], ['Négatif (0)', 'Positif (1)'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log dans MLFlow\n",
    "with mlflow.start_run(run_id=mlflow.active_run().info.run_id):\n",
    "    mlflow.log_artifact('bert_confusion_matrix.png')\n",
    "\n",
    "# Rapport de classification détaillé\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAPPORT DE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(val_labels.numpy(), val_pred_classes, \n",
    "                          target_names=['Négatif', 'Positif']))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Évaluation Finale sur le Set de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Évaluation finale sur le set de test...\")\n",
    "\n",
    "# Prédictions sur le test\n",
    "test_predictions = bert_model.predict(test_dataset)\n",
    "test_logits = test_predictions.logits\n",
    "test_pred_classes = np.argmax(test_logits, axis=1)\n",
    "test_probabilities = tf.nn.softmax(test_logits, axis=1).numpy()[:, 1]\n",
    "\n",
    "# Métriques finales\n",
    "test_metrics = {\n",
    "    'test_accuracy': accuracy_score(test_labels.numpy(), test_pred_classes),\n",
    "    'test_precision': precision_score(test_labels.numpy(), test_pred_classes),\n",
    "    'test_recall': recall_score(test_labels.numpy(), test_pred_classes),\n",
    "    'test_f1_score': f1_score(test_labels.numpy(), test_pred_classes),\n",
    "    'test_roc_auc': roc_auc_score(test_labels.numpy(), test_probabilities)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSULTATS FINAUX SUR LE SET DE TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy:  {test_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['test_precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['test_recall']:.4f}\")\n",
    "print(f\"F1-Score:  {test_metrics['test_f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC:   {test_metrics['test_roc_auc']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log dans MLFlow\n",
    "with mlflow.start_run(run_id=mlflow.active_run().info.run_id):\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dossier si nécessaire\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Sauvegarde du modèle complet\n",
    "model_path = '../models/bert_sentiment_model'\n",
    "bert_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print(f\"✓ Modèle BERT sauvegardé dans: {model_path}\")\n",
    "\n",
    "# Log dans MLFlow\n",
    "with mlflow.start_run(run_id=mlflow.active_run().info.run_id):\n",
    "    mlflow.log_artifact(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test de Prédiction sur des Exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Prédit le sentiment d'un texte.\n",
    "    \n",
    "    Returns:\n",
    "        sentiment: 'Positif' ou 'Négatif'\n",
    "        probability: Probabilité de la classe prédite\n",
    "    \"\"\"\n",
    "    # Tokenisation\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Prédiction\n",
    "    outputs = model(encoding)\n",
    "    logits = outputs.logits\n",
    "    probabilities = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "    \n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    sentiment = 'Positif' if predicted_class == 1 else 'Négatif'\n",
    "    confidence = probabilities[predicted_class]\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Tests sur des exemples\n",
    "test_examples = [\n",
    "    \"This flight was amazing! Best experience ever!\",\n",
    "    \"Terrible service, never flying with them again\",\n",
    "    \"The staff was friendly and helpful\",\n",
    "    \"Delayed for 5 hours, worst airline\",\n",
    "    \"Good value for money\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTS DE PRÉDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_examples:\n",
    "    sentiment, confidence = predict_sentiment(text, bert_model, tokenizer)\n",
    "    print(f\"\\nTexte: {text}\")\n",
    "    print(f\"Sentiment prédit: {sentiment} (confiance: {confidence:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Comparaison avec les Autres Modèles\n",
    "\n",
    "Cette section sera complétée après avoir exécuté tous les notebooks pour comparer:\n",
    "- Modèle simple (Logistic Regression)\n",
    "- Modèles avancés (Bi-LSTM, CNN avec Word2Vec/GloVe)\n",
    "- Modèle BERT\n",
    "\n",
    "Critères de comparaison:\n",
    "1. **Performance**: Accuracy, F1-Score, ROC-AUC\n",
    "2. **Temps d'entraînement**: Combien de temps pour entraîner?\n",
    "3. **Temps de prédiction**: Vitesse d'inférence\n",
    "4. **Complexité**: Nombre de paramètres, ressources nécessaires\n",
    "5. **Facilité de déploiement**: Taille du modèle, dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette cellule sera utilisée pour créer un tableau comparatif final\n",
    "# après avoir exécuté tous les notebooks\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ DES PERFORMANCES - BERT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nValidation Set:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nTemps d'entraînement: {training_time/60:.2f} minutes\")\n",
    "print(f\"Temps de prédiction: {prediction_time:.2f} secondes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a implémenté un modèle BERT pour la classification de sentiments de tweets.\n",
    "\n",
    "**Points clés:**\n",
    "- Utilisation de `bert-base-uncased` pré-entraîné\n",
    "- Fine-tuning sur notre dataset de tweets\n",
    "- Préparation appropriée des données (input_ids, attention_masks)\n",
    "- Tracking complet avec MLFlow\n",
    "\n",
    "**Prochaines étapes:**\n",
    "1. Comparer BERT avec les modèles précédents\n",
    "2. Choisir le meilleur modèle pour le déploiement\n",
    "3. Créer l'API pour le modèle sélectionné\n",
    "4. Déployer sur le Cloud\n",
    "5. Mettre en place le monitoring avec Azure Application Insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}