{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod√®le Avanc√© - Deep Learning avec Word Embeddings\n",
    "\n",
    "Ce notebook impl√©mente les mod√®les avanc√©s selon les crit√®res d'√©valuation :\n",
    "1. **Tester 2 pr√©traitements** : Lemmatization vs Stemming\n",
    "2. **Tester 2 word embeddings** : Word2Vec vs GloVe\n",
    "3. **2 architectures Deep Learning** : Bi-LSTM + CNN\n",
    "4. **Tracking MLFlow** complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 16:26:32.647959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU disponible: True\n"
     ]
    }
   ],
   "source": [
    "# Import des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
    "    Conv1D, GlobalMaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Gensim pour Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "\n",
    "# Joblib\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFlow configur√©\n",
      "Tracking URI: file:///home/thomas/mlruns\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLFlow\n",
    "mlflow.set_tracking_uri(\"file:///home/thomas/mlruns\")\n",
    "mlflow.set_experiment(\"sentiment-analysis-twitter\")\n",
    "\n",
    "print(\"MLFlow configur√©\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn√©es\n",
    "\n",
    "On charge **2 versions** du pr√©traitement pour comparer :\n",
    "- Lemmatization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es lemmatis√©es...\n",
      "‚úì Lemmatis√©es - Train: (969875, 2), Val: (207831, 2), Test: (207831, 2)\n",
      "\n",
      "Chargement des donn√©es stemm√©es...\n",
      "‚úì Stemm√©es - Train: (969875, 2), Val: (207831, 2), Test: (207831, 2)\n",
      "\n",
      "üöÄ MODE PRODUCTION: Utilisation de TOUTES les donn√©es\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° MODE D√âVELOPPEMENT: √âchantillonnage pour acc√©l√©rer l'entra√Ænement\n",
    "# Sans GPU, entra√Æner sur 970k tweets prend TR√àS longtemps (2-3h par mod√®le)\n",
    "SAMPLE_SIZE = None    # 50k tweets (~10-15min par mod√®le)\n",
    "# SAMPLE_SIZE = None   # Toutes les donn√©es (d√©commenter pour production)\n",
    "\n",
    "# Chargement des donn√©es LEMMATIS√âES\n",
    "print(\"Chargement des donn√©es lemmatis√©es...\")\n",
    "train_lemma = pd.read_csv('../data/processed/train_lemmatized.csv')\n",
    "val_lemma = pd.read_csv('../data/processed/val_lemmatized.csv')\n",
    "test_lemma = pd.read_csv('../data/processed/test_lemmatized.csv')\n",
    "\n",
    "print(f\"‚úì Lemmatis√©es - Train: {train_lemma.shape}, Val: {val_lemma.shape}, Test: {test_lemma.shape}\")\n",
    "\n",
    "# Chargement des donn√©es STEMM√âES\n",
    "print(\"\\nChargement des donn√©es stemm√©es...\")\n",
    "train_stem = pd.read_csv('../data/processed/train_stemmed.csv')\n",
    "val_stem = pd.read_csv('../data/processed/val_stemmed.csv')\n",
    "test_stem = pd.read_csv('../data/processed/test_stemmed.csv')\n",
    "\n",
    "print(f\"‚úì Stemm√©es - Train: {train_stem.shape}, Val: {val_stem.shape}, Test: {test_stem.shape}\")\n",
    "\n",
    "# √âchantillonnage si SAMPLE_SIZE est d√©fini\n",
    "if SAMPLE_SIZE:\n",
    "    print(f\"\\n‚ö° √âCHANTILLONNAGE √† {SAMPLE_SIZE} tweets pour acc√©l√©rer l'entra√Ænement...\")\n",
    "    print(f\"   (Mettre SAMPLE_SIZE = None pour utiliser toutes les donn√©es)\")\n",
    "    \n",
    "    # √âchantillonnage stratifi√© (garde la distribution 50/50)\n",
    "    train_lemma = train_lemma.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), SAMPLE_SIZE // 2), random_state=42)\n",
    "    )\n",
    "    train_stem = train_stem.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), SAMPLE_SIZE // 2), random_state=42)\n",
    "    )\n",
    "    \n",
    "    # R√©duction proportionnelle des sets val et test\n",
    "    val_size = int(SAMPLE_SIZE * 0.2)\n",
    "    test_size = int(SAMPLE_SIZE * 0.2)\n",
    "    \n",
    "    val_lemma = val_lemma.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), val_size // 2), random_state=42)\n",
    "    )\n",
    "    val_stem = val_stem.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), val_size // 2), random_state=42)\n",
    "    )\n",
    "    \n",
    "    test_lemma = test_lemma.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), test_size // 2), random_state=42)\n",
    "    )\n",
    "    test_stem = test_stem.groupby('sentiment', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), test_size // 2), random_state=42)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì APR√àS √©chantillonnage:\")\n",
    "    print(f\"  Lemma - Train: {train_lemma.shape}, Val: {val_lemma.shape}, Test: {test_lemma.shape}\")\n",
    "    print(f\"  Stem - Train: {train_stem.shape}, Val: {val_stem.shape}, Test: {test_stem.shape}\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ MODE PRODUCTION: Utilisation de TOUTES les donn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration des Param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  - Vocabulaire: 20000 mots\n",
      "  - Longueur max: 50 tokens\n",
      "  - Dimension embeddings: 100\n",
      "  - Batch size: 128\n",
      "  - Epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# Param√®tres globaux\n",
    "MAX_WORDS = 20000      # Taille du vocabulaire\n",
    "MAX_LEN = 50           # Longueur maximale des s√©quences\n",
    "EMBEDDING_DIM = 100    # Dimension des embeddings\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Vocabulaire: {MAX_WORDS} mots\")\n",
    "print(f\"  - Longueur max: {MAX_LEN} tokens\")\n",
    "print(f\"  - Dimension embeddings: {EMBEDDING_DIM}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des Donn√©es - Tokenisation\n",
    "\n",
    "On cr√©e **2 tokenizers** (un pour lemma, un pour stem) car les vocabulaires sont diff√©rents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©paration des donn√©es LEMMATIS√âES...\n",
      "‚úì Train: (969875, 50), Vocab: 213827\n",
      "\n",
      "Pr√©paration des donn√©es STEMM√âES...\n",
      "‚úì Train: (969875, 50), Vocab: 171700\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(train_df, val_df, test_df, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Pr√©pare les donn√©es : tokenisation et padding.\n",
    "    \"\"\"\n",
    "    X_train = train_df['text'].values\n",
    "    y_train = train_df['sentiment'].values\n",
    "    X_val = val_df['text'].values\n",
    "    y_val = val_df['sentiment'].values\n",
    "    X_test = test_df['text'].values\n",
    "    y_test = test_df['sentiment'].values\n",
    "    \n",
    "    # Tokenization\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Conversion en s√©quences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Padding\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    return (X_train_pad, y_train), (X_val_pad, y_val), (X_test_pad, y_test), tokenizer\n",
    "\n",
    "# Pr√©paration des donn√©es lemmatis√©es\n",
    "print(\"Pr√©paration des donn√©es LEMMATIS√âES...\")\n",
    "(X_train_lemma, y_train_lemma), (X_val_lemma, y_val_lemma), (X_test_lemma, y_test_lemma), tokenizer_lemma = prepare_data(\n",
    "    train_lemma, val_lemma, test_lemma\n",
    ")\n",
    "print(f\"‚úì Train: {X_train_lemma.shape}, Vocab: {len(tokenizer_lemma.word_index)}\")\n",
    "\n",
    "# Pr√©paration des donn√©es stemm√©es\n",
    "print(\"\\nPr√©paration des donn√©es STEMM√âES...\")\n",
    "(X_train_stem, y_train_stem), (X_val_stem, y_val_stem), (X_test_stem, y_test_stem), tokenizer_stem = prepare_data(\n",
    "    train_stem, val_stem, test_stem\n",
    ")\n",
    "print(f\"‚úì Train: {X_train_stem.shape}, Vocab: {len(tokenizer_stem.word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©ation des Word Embeddings\n",
    "\n",
    "On cr√©e **2 types d'embeddings** :\n",
    "1. **Word2Vec** (entra√Æn√© sur nos donn√©es)\n",
    "2. **GloVe** (pr√©-entra√Æn√©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra√Ænement Word2Vec (lemmatis√©)...\n",
      "‚úì Word2Vec lemma - Vocabulaire: 81797 mots\n",
      "\n",
      "Entra√Ænement Word2Vec (stemm√©)...\n",
      "‚úì Word2Vec stem - Vocabulaire: 66406 mots\n"
     ]
    }
   ],
   "source": [
    "# Entra√Ænement Word2Vec sur donn√©es lemmatis√©es\n",
    "print(\"Entra√Ænement Word2Vec (lemmatis√©)...\")\n",
    "tokenized_texts_lemma = [text.split() for text in train_lemma['text'].values]\n",
    "\n",
    "w2v_model_lemma = Word2Vec(\n",
    "    sentences=tokenized_texts_lemma,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sg=1  # Skip-gram\n",
    ")\n",
    "print(f\"‚úì Word2Vec lemma - Vocabulaire: {len(w2v_model_lemma.wv)} mots\")\n",
    "\n",
    "# Entra√Ænement Word2Vec sur donn√©es stemm√©es\n",
    "print(\"\\nEntra√Ænement Word2Vec (stemm√©)...\")\n",
    "tokenized_texts_stem = [text.split() for text in train_stem['text'].values]\n",
    "\n",
    "w2v_model_stem = Word2Vec(\n",
    "    sentences=tokenized_texts_stem,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sg=1\n",
    ")\n",
    "print(f\"‚úì Word2Vec stem - Vocabulaire: {len(w2v_model_stem.wv)} mots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 GloVe (Pr√©-entra√Æn√©)\n",
    "\n",
    "T√©l√©charger GloVe : https://nlp.stanford.edu/projects/glove/  \n",
    "Utiliser : `glove.6B.100d.txt` (100 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement GloVe depuis ../data/glove.6B.100d.txt...\n",
      "‚úì GloVe charg√© - 400000 mots\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    \"\"\"\n",
    "    Charge les embeddings GloVe depuis un fichier.\n",
    "    \"\"\"\n",
    "    print(f\"Chargement GloVe depuis {glove_path}...\")\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        \n",
    "        print(f\"‚úì GloVe charg√© - {len(embeddings_index)} mots\")\n",
    "        return embeddings_index\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Fichier GloVe non trouv√© !\")\n",
    "        print(\"T√©l√©charge depuis : https://nlp.stanford.edu/projects/glove/\")\n",
    "        print(\"Fichier : glove.6B.100d.txt\")\n",
    "        return None\n",
    "\n",
    "# Tentative de chargement GloVe\n",
    "glove_path = '../data/glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "if glove_embeddings is None:\n",
    "    print(\"\\n‚ö†Ô∏è GloVe non disponible. On utilisera Word2Vec uniquement.\")\n",
    "    print(\"Pour t√©l√©charger GloVe :\")\n",
    "    print(\"  1. wget http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "    print(\"  2. unzip glove.6B.zip\")\n",
    "    print(\"  3. mv glove.6B.100d.txt ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cr√©ation des Matrices d'Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr√©ation matrices Word2Vec...\n",
      "  Lemma:\n",
      "  Couverture: 19998/20000 (100.0%)\n",
      "  Stem:\n",
      "  Couverture: 19998/20000 (100.0%)\n",
      "\n",
      "Cr√©ation matrices GloVe...\n",
      "  Lemma:\n",
      "  Couverture: 17033/20000 (85.2%)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix_w2v(word_index, w2v_model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Cr√©e une matrice d'embeddings depuis Word2Vec.\n",
    "    \"\"\"\n",
    "    vocab_size = min(len(word_index) + 1, MAX_WORDS)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    found_words = 0\n",
    "    for word, idx in word_index.items():\n",
    "        if idx >= MAX_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = w2v_model.wv[word]\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            found_words += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    coverage = found_words / min(len(word_index), MAX_WORDS) * 100\n",
    "    print(f\"  Couverture: {found_words}/{min(len(word_index), MAX_WORDS)} ({coverage:.1f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_embedding_matrix_glove(word_index, glove_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Cr√©e une matrice d'embeddings depuis GloVe.\n",
    "    \"\"\"\n",
    "    vocab_size = min(len(word_index) + 1, MAX_WORDS)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    found_words = 0\n",
    "    for word, idx in word_index.items():\n",
    "        if idx >= MAX_WORDS:\n",
    "            continue\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            found_words += 1\n",
    "    \n",
    "    coverage = found_words / min(len(word_index), MAX_WORDS) * 100\n",
    "    print(f\"  Couverture: {found_words}/{min(len(word_index), MAX_WORDS)} ({coverage:.1f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Cr√©ation des matrices Word2Vec\n",
    "print(\"Cr√©ation matrices Word2Vec...\")\n",
    "print(\"  Lemma:\")\n",
    "emb_matrix_w2v_lemma = create_embedding_matrix_w2v(tokenizer_lemma.word_index, w2v_model_lemma, EMBEDDING_DIM)\n",
    "print(\"  Stem:\")\n",
    "emb_matrix_w2v_stem = create_embedding_matrix_w2v(tokenizer_stem.word_index, w2v_model_stem, EMBEDDING_DIM)\n",
    "\n",
    "# Cr√©ation des matrices GloVe si disponible\n",
    "if glove_embeddings:\n",
    "    print(\"\\nCr√©ation matrices GloVe...\")\n",
    "    print(\"  Lemma:\")\n",
    "    emb_matrix_glove_lemma = create_embedding_matrix_glove(tokenizer_lemma.word_index, glove_embeddings, EMBEDDING_DIM)\n",
    "else:\n",
    "    emb_matrix_glove_lemma = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. D√©finition des Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architectures d√©finies:\n",
      "  ‚úì Bi-LSTM\n",
      "  ‚úì CNN\n"
     ]
    }
   ],
   "source": [
    "def build_bilstm_model(vocab_size, embedding_matrix):\n",
    "    \"\"\"\n",
    "    Architecture Bi-LSTM avec embeddings pr√©-entra√Æn√©s.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False  # Freeze embeddings\n",
    "        ),\n",
    "        Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_cnn_model(vocab_size, embedding_matrix):\n",
    "    \"\"\"\n",
    "    Architecture CNN avec embeddings pr√©-entra√Æn√©s.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False\n",
    "        ),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Architectures d√©finies:\")\n",
    "print(\"  ‚úì Bi-LSTM\")\n",
    "print(\"  ‚úì CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fonction d'Entra√Ænement et d'√âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                      model_name, preprocessing, embedding_type):\n",
    "    \"\"\"\n",
    "    Entra√Æne et √©value un mod√®le avec tracking MLFlow.\n",
    "    \"\"\"\n",
    "    run_name = f\"{model_name}_{embedding_type}_{preprocessing}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ENTRA√éNEMENT: {run_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        # Log des param√®tres\n",
    "        mlflow.log_param(\"model_architecture\", model_name)\n",
    "        mlflow.log_param(\"embedding_type\", embedding_type)\n",
    "        mlflow.log_param(\"preprocessing\", preprocessing)\n",
    "        mlflow.log_param(\"embedding_dim\", EMBEDDING_DIM)\n",
    "        mlflow.log_param(\"max_words\", MAX_WORDS)\n",
    "        mlflow.log_param(\"max_len\", MAX_LEN)\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        start_time = time()\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        training_time = time() - start_time\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        y_test_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "        y_test_proba = model.predict(X_test).flatten()\n",
    "        \n",
    "        # M√©triques\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'precision': precision_score(y_test, y_test_pred),\n",
    "            'recall': recall_score(y_test, y_test_pred),\n",
    "            'f1': f1_score(y_test, y_test_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_test_proba)\n",
    "        }\n",
    "        \n",
    "        # Log m√©triques\n",
    "        for metric_name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"test_{metric_name}\", value)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"\\nR√âSULTATS - TEST SET:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  Temps:     {training_time:.2f}s\")\n",
    "        \n",
    "        # Courbes d'apprentissage\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        ax1.plot(history.history['loss'], label='Train Loss')\n",
    "        ax1.plot(history.history['val_loss'], label='Val Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = f'training_{run_name}.png'\n",
    "        plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        mlflow.log_artifact(plot_filename)\n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "        \n",
    "        return model, metrics, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement des Mod√®les\n",
    "\n",
    "### Plan d'exp√©rimentation :\n",
    "\n",
    "1. **Bi-LSTM + Word2Vec + Lemma** (baseline)\n",
    "2. **Bi-LSTM + Word2Vec + Stem** ‚Üí Compare preprocessing\n",
    "3. **Bi-LSTM + GloVe + Lemma** ‚Üí Compare embeddings\n",
    "4. **CNN + GloVe + Lemma** ‚Üí Compare architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des r√©sultats\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Bi-LSTM + Word2Vec + Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766849420.096436   37394 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5527 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENTRA√éNEMENT: BiLSTM_Word2Vec_Lemmatization\n",
      "================================================================================\n",
      "Epoch 1/3\n",
      "\u001b[1m 240/7578\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:56:49\u001b[0m 955ms/step - accuracy: 0.6267 - loss: 0.6380"
     ]
    }
   ],
   "source": [
    "vocab_size_lemma = min(len(tokenizer_lemma.word_index) + 1, MAX_WORDS)\n",
    "model_1 = build_bilstm_model(vocab_size_lemma, emb_matrix_w2v_lemma)\n",
    "\n",
    "model_1, metrics_1, history_1 = train_and_evaluate(\n",
    "    model_1,\n",
    "    X_train_lemma, y_train_lemma,\n",
    "    X_val_lemma, y_val_lemma,\n",
    "    X_test_lemma, y_test_lemma,\n",
    "    \"BiLSTM\", \"Lemmatization\", \"Word2Vec\"\n",
    ")\n",
    "\n",
    "results['BiLSTM_Word2Vec_Lemma'] = metrics_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Bi-LSTM + Word2Vec + Stemming\n",
    "\n",
    "**Objectif** : Comparer lemmatization vs stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_stem = min(len(tokenizer_stem.word_index) + 1, MAX_WORDS)\n",
    "model_2 = build_bilstm_model(vocab_size_stem, emb_matrix_w2v_stem)\n",
    "\n",
    "model_2, metrics_2, history_2 = train_and_evaluate(\n",
    "    model_2,\n",
    "    X_train_stem, y_train_stem,\n",
    "    X_val_stem, y_val_stem,\n",
    "    X_test_stem, y_test_stem,\n",
    "    \"BiLSTM\", \"Stemming\", \"Word2Vec\"\n",
    ")\n",
    "\n",
    "results['BiLSTM_Word2Vec_Stem'] = metrics_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Bi-LSTM + GloVe + Lemmatization\n",
    "\n",
    "**Objectif** : Comparer Word2Vec vs GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glove_embeddings:\n",
    "    model_3 = build_bilstm_model(vocab_size_lemma, emb_matrix_glove_lemma)\n",
    "    \n",
    "    model_3, metrics_3, history_3 = train_and_evaluate(\n",
    "        model_3,\n",
    "        X_train_lemma, y_train_lemma,\n",
    "        X_val_lemma, y_val_lemma,\n",
    "        X_test_lemma, y_test_lemma,\n",
    "        \"BiLSTM\", \"Lemmatization\", \"GloVe\"\n",
    "    )\n",
    "    \n",
    "    results['BiLSTM_GloVe_Lemma'] = metrics_3\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GloVe non disponible, mod√®le 3 ignor√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 CNN + GloVe + Lemmatization\n",
    "\n",
    "**Objectif** : Comparer Bi-LSTM vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glove_embeddings:\n",
    "    model_4 = build_cnn_model(vocab_size_lemma, emb_matrix_glove_lemma)\n",
    "    \n",
    "    model_4, metrics_4, history_4 = train_and_evaluate(\n",
    "        model_4,\n",
    "        X_train_lemma, y_train_lemma,\n",
    "        X_val_lemma, y_val_lemma,\n",
    "        X_test_lemma, y_test_lemma,\n",
    "        \"CNN\", \"Lemmatization\", \"GloVe\"\n",
    "    )\n",
    "    \n",
    "    results['CNN_GloVe_Lemma'] = metrics_4\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GloVe non disponible, mod√®le 4 ignor√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparaison et Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON DES MOD√àLES - TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_model_name = results_df['f1'].idxmax()\n",
    "best_f1 = results_df['f1'].max()\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Comparaison des Performances - Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'gold', 'mediumpurple']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    results_df[metric].plot(kind='barh', ax=ax, color=color, edgecolor='black')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(metric.upper())\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.set_xlim([0, 1])\n",
    "\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models_comparison_final.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des Comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DES COMPARAISONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comparaison Lemma vs Stem\n",
    "if 'BiLSTM_Word2Vec_Lemma' in results and 'BiLSTM_Word2Vec_Stem' in results:\n",
    "    print(\"\\n1. LEMMATIZATION vs STEMMING (Bi-LSTM + Word2Vec):\")\n",
    "    print(f\"   Lemma F1: {results['BiLSTM_Word2Vec_Lemma']['f1']:.4f}\")\n",
    "    print(f\"   Stem F1:  {results['BiLSTM_Word2Vec_Stem']['f1']:.4f}\")\n",
    "    diff = results['BiLSTM_Word2Vec_Lemma']['f1'] - results['BiLSTM_Word2Vec_Stem']['f1']\n",
    "    winner = \"Lemmatization\" if diff > 0 else \"Stemming\"\n",
    "    print(f\"   ‚Üí {winner} gagne avec +{abs(diff):.4f} points\")\n",
    "\n",
    "# Comparaison Word2Vec vs GloVe\n",
    "if 'BiLSTM_Word2Vec_Lemma' in results and 'BiLSTM_GloVe_Lemma' in results:\n",
    "    print(\"\\n2. WORD2VEC vs GLOVE (Bi-LSTM + Lemmatization):\")\n",
    "    print(f\"   Word2Vec F1: {results['BiLSTM_Word2Vec_Lemma']['f1']:.4f}\")\n",
    "    print(f\"   GloVe F1:    {results['BiLSTM_GloVe_Lemma']['f1']:.4f}\")\n",
    "    diff = results['BiLSTM_Word2Vec_Lemma']['f1'] - results['BiLSTM_GloVe_Lemma']['f1']\n",
    "    winner = \"Word2Vec\" if diff > 0 else \"GloVe\"\n",
    "    print(f\"   ‚Üí {winner} gagne avec +{abs(diff):.4f} points\")\n",
    "\n",
    "# Comparaison Bi-LSTM vs CNN\n",
    "if 'BiLSTM_GloVe_Lemma' in results and 'CNN_GloVe_Lemma' in results:\n",
    "    print(\"\\n3. BI-LSTM vs CNN (GloVe + Lemmatization):\")\n",
    "    print(f\"   Bi-LSTM F1: {results['BiLSTM_GloVe_Lemma']['f1']:.4f}\")\n",
    "    print(f\"   CNN F1:     {results['CNN_GloVe_Lemma']['f1']:.4f}\")\n",
    "    diff = results['BiLSTM_GloVe_Lemma']['f1'] - results['CNN_GloVe_Lemma']['f1']\n",
    "    winner = \"Bi-LSTM\" if diff > 0 else \"CNN\"\n",
    "    print(f\"   ‚Üí {winner} gagne avec +{abs(diff):.4f} points\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dossier models\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Sauvegarde\n",
    "print(f\"\\nSauvegarde du meilleur mod√®le: {best_model_name}\")\n",
    "\n",
    "# Map des mod√®les\n",
    "models_dict = {}\n",
    "if 'model_1' in locals(): models_dict['BiLSTM_Word2Vec_Lemma'] = model_1\n",
    "if 'model_2' in locals(): models_dict['BiLSTM_Word2Vec_Stem'] = model_2\n",
    "if 'model_3' in locals(): models_dict['BiLSTM_GloVe_Lemma'] = model_3\n",
    "if 'model_4' in locals(): models_dict['CNN_GloVe_Lemma'] = model_4\n",
    "\n",
    "best_model = models_dict[best_model_name]\n",
    "best_model.save('../models/best_deep_learning_model.h5')\n",
    "\n",
    "# Sauvegarde du tokenizer appropri√©\n",
    "if 'Lemma' in best_model_name:\n",
    "    joblib.dump(tokenizer_lemma, '../models/tokenizer.pkl')\n",
    "else:\n",
    "    joblib.dump(tokenizer_stem, '../models/tokenizer.pkl')\n",
    "\n",
    "# M√©tadonn√©es\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': results_df.loc[best_model_name].to_dict(),\n",
    "    'max_words': MAX_WORDS,\n",
    "    'max_len': MAX_LEN,\n",
    "    'embedding_dim': EMBEDDING_DIM\n",
    "}\n",
    "joblib.dump(metadata, '../models/model_metadata.pkl')\n",
    "\n",
    "print(\"‚úì Mod√®le sauvegard√©: ../models/best_deep_learning_model.h5\")\n",
    "print(\"‚úì Tokenizer sauvegard√©: ../models/tokenizer.pkl\")\n",
    "print(\"‚úì Metadata sauvegard√©: ../models/model_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"R√âSUM√â FINAL - MOD√àLES AVANC√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ CRIT√àRES D'√âVALUATION RESPECT√âS:\")\n",
    "print(f\"   ‚úì 2 pr√©traitements test√©s (Lemmatization vs Stemming)\")\n",
    "print(f\"   ‚úì 2 word embeddings test√©s (Word2Vec vs GloVe)\")\n",
    "print(f\"   ‚úì 2 architectures Deep Learning (Bi-LSTM + CNN)\")\n",
    "print(f\"   ‚úì Au moins 1 mod√®le avec LSTM\")\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE:\")\n",
    "print(f\"   Nom: {best_model_name}\")\n",
    "print(f\"   Performances (Test Set):\")\n",
    "for metric, value in results_df.loc[best_model_name].items():\n",
    "    print(f\"     - {metric:10s}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä NOMBRE DE MOD√àLES ENTRA√éN√âS: {len(results)}\")\n",
    "print(f\"\\nüìÅ FICHIERS SAUVEGARD√âS:\")\n",
    "print(f\"   ‚úì ../models/best_deep_learning_model.h5\")\n",
    "print(f\"   ‚úì ../models/tokenizer.pkl\")\n",
    "print(f\"   ‚úì ../models/model_metadata.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MOD√àLES AVANC√âS COMPL√âT√âS !\")\n",
    "print(\"Prochaine √©tape: Mod√®le BERT\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
